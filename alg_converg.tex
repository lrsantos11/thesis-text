%!TEX root = tese.tex
\section{Convergência do Algoritmo de Escolha Adiada de Parâmetros}



Vamos provar  a convergência do Algoritmo \ref{alg:optimized-choice-of-parameters}. 

A fim de facilitar a compreensão, fixaremos os parâmetros $(\mu,\sig)$. Neste sentido escolheremos para o parâmetro  $\mu$ o valor \[\bar{\mu}=\eta\dfrac{x^Tz}{n},
\] 
em que $\eta\in [\eta_{\min},\eta_{\max} ]$ enquanto o parâmetro $\sig$ será fixado como $\bar{\sig} = 0$. Essas escolhas  podem fazer com que nosso método, resolva  sistemas linerares similares aos dos métodos seguidores de caminho, vistos no Capítulo~\ref{chap:mpis}.  No entanto, para fins de clareza, e sem perda de generalidade, vamos escolher $\eta_{\min}=\eta_{\max}=\eta $ como em  
\cite{Zhang:1995fu}.


 Na prática, porém, esperamos que  o método  tenha desempenho melhor do que a convergência teórica, já que se buscará, em cada iteração $k$,
o minimizador global de $\varphi_k$, dado por $(\al^*,\mu^*,\sig^*)$ e portanto	$\varphi_k(\al^*,\mu^*,\sig^*)\leq \varphi_k(\al^*,\bar{\mu},\bar{\sig})$.


Com essas alterações, podemos reescrever o Algoritmo~\ref{alg:optimized-choice-of-parameters}, como o  Algoritmo Simplificado \ref{alg:optimized-choice-of-parameters-simplified}.


A fim de pode  resolver o o problema primal-dual (\ref{eq:primal}-\ref{eq:dual}), vamos considerar válido o Pressuposto \ref{ass:interior-nonempty}, isto é, que o interior da região factível é não vazio. Além disso, vamos considerar que o \acl{PL} em questão tem ao menos uma solução ótima. 


Com isso, vamos considerar a relação existente entre o resíduo linear escalado $\rho_L$ na iteração $k$ com o resíduo 
linear da iteração $k+1$, dada no Teorema \ref{thm:next=residual}. É possível, generalizar tal relação usando a seguinte proposição. 
\begin{prop} Seja $\{(x^k,y^k,z^k)\}$ gerada pelo Algoritmo \ref{alg:optimized-choice-of-parameters-simplified}. Então para $k\geq0$, vale  
\begin{equation}
\label{eq:residual-norm-relation}
% \norm{(\rho_P^{k+1},\rho_D^{k+1})}_1  = (1-\al_k) \norm{(\rho_P^{k},\rho_D^{k})}_1 = \nu_{k+1} \norm{(\rho_P^{0},\rho_D^{0})}_1,
 \rho_L^{k+1}  = (1-\al_k) \rho_L^{k} = \nu_{k+1}  \rho_L^{0},
\end{equation}
em que  $\nu_0 = 1$ e 
\[
\nu_{k+1} = (1-\al_k)\nu_k = \prod_{j=0}^{k}(1-\al_j) \geq 0.
\]
\end{prop}

\begin{proof}
	Usando a primeira equação \eqref{eq:next-residual-all} do Teorema \ref{thm:next=residual},  temos que  o resíduo linear na iteração $k$ será dado por $\rho_L^{k} = (1-\al)\rho_L^{k}$, em que $\rho_L^{k} = (\rho_P^{k},\rho_D^{k})$. Assim, 
	\begin{align*}
		\rho_L^{k} & = (1-\al_{k-1})\rho_L^{k-1}\notag\\
				   & = (1-\al_{k-1})\cdots(1-\al_{0})\rho_L^{0}\notag\\
				   & = \nu_k \rho_L^{0}.
	\end{align*}
\end{proof}
Como  $\nu_k$ é uma constante não negativa,  uma consequência desta proposição é que se $\rho_L^0\neq 0 $, para toda norma-p, vale 
\[
\nu_k = \frac{\norm{\rho_L^{k}}_p}{\norm{\rho_L^{0}}_p},
\]
e logo
\[
\nu_k = \frac{\dbvec{\rho_L}_k}{\dbvec{\rho_L}_0}.
\]


Usando a proposição acima, a função de mérito para o próximo ponto --  eliminando o índice da iteração $k$ e considerando o próximo ponto $(\hat{x},\hat{y},\hat{z})$ --  é, nas variáveis  $(\al,\mu,\sig)$,
\begin{equation}
% \label{eq:merit-function-al-mu-sig}
{\varphi}(\al,\mu,\sig) = (1-\al)(\dbvec{\rho_L} +
\dbvec{\rho_C}) + \al\mu + \al(\al-\sig)\dbvec{L_{0,0}} +
\al^2\dbvec{\Lambda(\mu,\sig)} ,
\end{equation}
em que 
\[
\dbvec{\Lambda(\mu,\sig)} = \mu^2
 \dbvec{L_{2,0}} + \mu \dbvec{L_{1,0}} + 	\mu \sig \dbvec{L_{1,1}} +
 \sig^2 \dbvec{L_{0,2}} + \sig \dbvec{L_{0,1}}.
 \]
Como vimos, $\dbvec{L_{2,0}} = \dbvec{L_{0,2}} = 0$, e escolhendo $\mu = \eta\frac{x^Tz}{n} = \eta\dbvec{\rho_C} $
 e $\sig=0$ podemos reescrever a função de mérito para o próximo ponto, apenas dependendo de uma escolha de  $\al$, como
\[
% \label{eq:merit-function-al-mu-sig}
{\varphi}_{k+1}(\al)  = (1-\al)(\nu_{k}\dbvec{\rho_L}_0 +
\dbvec{\rho_C}_{k}) + \al\eta\dbvec{\rho_C}_{k} + \al^2\left(\dbvec{L_{0,0}}_{k} + \eta\dbvec{\rho_C}_{k} \dbvec{L_{1,0}}_{k}
\right) .
\]
Seja 
\begin{equation}
	\label{eq:theta}
\theta(\al) =  \dfrac{\al\left[ \nu_{k}\dbvec{\rho_L}_0 + (1-\eta)\dbvec{\rho_C}_{k} + \al\left(\dbvec{L_{0,0}}_{k} + \eta\dbvec{\rho_C}_{k} \dbvec{L_{1,0}}_{k}
\right) \right]}{\nu_{k}\dbvec{\rho_L}_0 +
\dbvec{\rho_C}_{k}}.
\end{equation}
Com esta definição, podemos escrever a seguinte relação entre a função de mértio atual e a próxima:
\begin{equation}
	\label{eq:relation-phi-next-phi}
	 			{\varphi}_{k+1} = (1- \theta(\al))\varphi_{k}
\end{equation}

Como quer-se que $\varphi_{k+1} $  e $\varphi_{k} $ sejam positivos, em toda iteração $k$, precisa-se escolher $\al_k$ tal que  $\theta_k = \theta(\al_k)<1$. Além disso, se existir $\theta>0$, tal que $\theta = \liminf \theta_k$, isto é, se a sequência $\{\theta_k\}$ for limitada inferiormente por um valor positivo, então a sequência $\{\varphi_k\}$, gerada pelo Algoritmo \ref{alg:optimized-choice-of-parameters-simplified}, converge para zero Q-linearmente.


Como quer-se garantir que $\varphi(\al) \leq  0$, então $\theta(\al)$ deve ser menor que 1. 

% Com efeito, observe que como $\eta<1$, então $0<\theta'(0)<1$ e por continuidade, temos que é possível escolher $\al_k\in(0,1]$ tal que 
% \[
% 0 < \theta_k = \theta(\al_k) < 1.
% \]

 Em resumo, para garantirmos a convergência do método, é crucial que exista $\bar{\al}>0$  tal que,  para todo $\al_k\in(0,\bar{\al}]$ em toda iteração $k$, a equação \eqref{eq:relation-phi-next-phi} seja válida e  e além disso que o próximo ponto  $({x}^{k+1} ,{y}^{k+1},{z}^{k+1})$ pertença a vizinhança $\Nset_{-\infty}(\gamma,\beta)$.
 Essa propriedade garante que  o Algoritmo \ref{alg:optimized-choice-of-parameters-simplified} gere uma sequência $\{\varphi_k\}$ que decresce de maneira significante a cada passo e convirja para zero.



Vamos escolher um $\gamma\in(0,1)$ adequado, para construir a vizinhança. Várias escolhas são possíveis, como, por exemplo a de \textcite{Colombo:2008ia} que fazem $\ga = 1/10$. No entanto, vamos utilizar a estratégia de \textcite{Zhang:2006ic}
\[
\gamma \leq \frac{\min(x^0z^0)}{(x^0)^Tz^0/n},
\]
que garante que o ponto inicial estará dentro da vizinhança. Mais detalhes sobre a escolha de $\gamma$ será feita na Capítulo~\ref{chap:numerical}, que versa a respeito da implementação.

Considerando que fixamos o valor de $\mu$ e de $\sig$, para minimizar a função de mérito para o próximo ponto tanto quanto possível em cada iteração, sujeita as restrições da Equações\eqref{eq:symmetric-polynomials-a} e \eqref{eq:symmetric-polynomials-b} dadas pelo Teorema~\ref{thm:polynomial-constraints} -- restrições que garantem que o próximo ponto está em $\Nset_{-\infty}(\gamma,\beta)$ --, escolhemos o tamanho de passo $\bar{\al}$ como

\begin{equation}
\label{eq:bound-alpha}
	\bar{\al} = \arg\max \{\theta(\al):\al\in[0,\min\{\al_L,\al_C\}] \}
\end{equation}





  \begin{algorithm}[htb]
 \onehalfspacing
 \caption{Método de Escolha Adiada Simplificado.}
 \label{alg:optimized-choice-of-parameters-simplified} 
\begin{algorithmic}[1]
\Procedure{ResolveLP}{$A,b,c$}
\State $(x^0,y^0,z^0) \gets$ \Call{PontoInicial}{$A,b,c$}.
\Comment{Assegure que  $(x^0,z^0)>0$ e que $\eta\in(0,1)$}
	\For {$k=1,2,\ldots$}
		\State Encontre		$((\dex)^{k},(\dey)^{k},(\dez)^{k})$ resolvendo
				\begin{equation}
				\label{eq:predictor-linear-matrix-simplified}
				\bbm A & 0 & 0 \\
				0 & A^T & I\\
				Z^k & 0 & X^k \ebm
				\bbm (\dex)^k \\ (\dey)^k \\ (\dez)^k
				\ebm = 
				\bbm -r_P^k  \\ -r_D^k \\ -r_C^k
				\ebm.
			\end{equation}
		\State 	Faça $\bar{\mu}=\eta{(x^k)^Tz^k}/{n}$. Fazendo $\bar{\sig}=0$ , encontre 
		\[((\Decox)^{k},(\Decoy)^{k},(\Decoz)^{k}) = \bar{\mu}((\Dex^{\mu})^{k},(\Dey^{\mu})^{k},(\Dez^{\mu})^{k})\]
		resolvendo
			\begin{equation}
				\label{eq:corrector-linear-matrix-simplified}
				\bbm A & 0 & 0 \\
				0 & A^T & I\\
				Z^k & 0 & X^k \ebm
				\bbm (\Dex^{\mu})^{k} \\ (\Dey^{\mu})^{k} \\ (\Dez^{\mu})^{k}
				\ebm = 
				\bbm 0  \\ 0 \\  e %- \bar{\sig}\deX\dez
				\ebm.
			\end{equation}
		\State Calcule  $\hat{\varphi}(\al_{k},\bar{\mu},0)$ usando
		\eqref{eq:merit-function-al-mu-sig} e $\psi(\al_{k},\bar{\mu},0)$ usando
		\eqref{eq:explicit-symmetric-neighbourhood}.
		\State Encontre $(\al_{k},\bar{\mu},0)$ resolvendo o subproblema de
		otimização global 
		\eqref{eq:pop-subproblem}.		
		\State Garanta que $\al_k\in(0,1)$ tal que $(x^{k+1},z^{k+1})>0$ e faça
		\[
		\begin{aligned}	
		& x^{k+1} = x^{k} + \al_k((\dex)^{k} + (\Decox)^{k} )
		\\
		& y^{k+1} = y^{k} + \al_k((\dey)^{k} + (\Decoy)^{k} )
		\\
		& z^{k+1} = z^{k} + \al_k((\dez)^{k} + (\Decoz)^{k} )
		 \end{aligned}. 
		\]		
	\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
