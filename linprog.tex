
\setchapterpreamble[u]{% 
\dictum[George B.~Dantzig]{``Linear programming can be viewed as part of a great
revolutionary development which has given mankind the ability to state general
goals and to lay out a path of detailed decisions to take in order to
 `best' achieve its goals when faced with practical situations of great
complexity.''~\cite{Dantzig:2002td}}
}

\chapter{Programação linear}  
   
\label{chap:linprog}

   Neste capítulo será feita uma breve introdução aos problemas de
\acrodef{PL}{Programação Linear} \ac{PL}, bem como a aspectos preliminares de
alguns métodos desenvolvidos para resolvê-los. 

\acl{PL} é um tópico que tem sido amplamente estudado no escopo da otimização e
que ganhou relevância a partir da década de 1940 com os trabalhos de Dantzig,
Kantorovich, Koopmans e von Neumann. A criação do método simplex por Dantzig 
acelerou o interesse pelo tema, já que se encontrou um meio de resolver tal problema. O texto de Dantzig que serve de epígrafe a este capítulo  serve ao mesmo tempo como um resumo histórico do
desenvolvimento da \ac{PL} e a define de maneira genérica.


A partir da necessidade de tratar matematicamente problemas econômicos
e de planejamento militar nos anos que se seguiram à Segunda Guerra Mundial, ao
mesmo tempo em que os computadores passaram a poder realizar operações
matemáticas com maior velocidade, houve um crescimento espantoso da utilização
da \ac{PL} e, por extensão, da otimização em geral. Para um resumo histórico há
boas fontes
\cite{Schrijver:Theory-of-Linear:1986k,Dantzig:Linear-Programming:1963t}.

Um problema de otimização pode ser descrito em termos de variáveis de decisão,
conjuntos de restrições e uma função objetivo. Resolver um problema de
otimização significa encontrar a ``melhor maneira'' na qual as variáveis
satisfaçam as restrições ao mesmo tempo em que se otimiza -- 
minimiza ou maximiza --  a função objetivo. 
 
Um problema de \emph{\acl{PL}}  é um problema de otimização no qual as
restrições e a função objetivo são lineares. A \ac{PL}\footnote{Usaremos
os termos ``problema de PL'' e ``PL'' como sinônimos.} surge diretamente em várias aplicações
reais, por exemplo, em problemas de economia, logística, planejamento e controle
da produção, entre outros. Ela aparece ainda em aproximações de problemas mais
complexos ou na solução relaxada de problemas de programação inteira ou mista.


    
Matematicamente, podemos escrever qualquer \ac{PL} na seguinte \emph{forma
padrão}:
\begin{equation} \label{eq:introPL-primal}
	\begin{array}{lc}
\displaystyle \min_{x} & c^Tx \\
\text{sujeito a} &\begin{cases} Ax = b \\
				 x \geq 0	
				 \end{cases}
\end{array},
\end{equation}
em que $A\in \Real^{m\times n}$ e $c$, $x$ e $b$ têm dimensões compatíveis. A
região $\Pset = \{x\in\Real^n | Ax=b \text{ e } x\geq 0\}$ é conhecida como
região factível do problema e é um poliedro com pelo menos um vértice ou
ponto extremo.

Como uma função linear é convexa, problemas de \ac{PL} têm a particularidade
dada pelo seguinte
teorema~\cite[cap.~3]{Bazaraa:2009uu}:

\begin{teo}[Teorema Fundamental da \acl{PL}] \label{teo:fundamental-pl} Em um
problema de \acl{PL} com região factível $\Pset$, ou o valor ótimo da função objetivo é  ilimitado ou então este valor
será atingido em um ponto extremo de  $\Pset$.
\end{teo}


Um conjunto de restrições lineares define um \emph{poliedro} que constitui
então a \emph{região factível}. De acordo com o
Teorema~\ref{teo:fundamental-pl}, uma forma de encontrar a solução de um
problema de \ac{PL} seria percorrer todos os vértices da região factível
comparando os valores da função objetivo e selecionando o melhor dentre eles.

Essa estratégia, no entanto, é pouco eficiente, haja vista o poliedro ser
resultante de um sistema linear de	$m$ restrições e $n$  variáveis $(m<n)$
podendo ter um número de vértices totalizando
\[ 
\binom{n}{m} = \dfrac{n!}{m!(n-m)!} \geq 
\left(\dfrac{n}{m}\right)^m,
\]
quando o problema está na forma padrão. Por conta disso, um método do tipo
exaustão seria pouco eficiente. Por outro lado, o fato de o número de vértices
ser limitado garante terminação finita para qualquer método que funcione desta
forma. Entretanto, esse número é exponencial,
já que 
\[
\left(\dfrac{n}{m}\right)^m \geq 2^m  \text{ para }   n\geq m.
\]
Encontrar um \emph{vértice ótimo} de maneira  eficiente é o
segredo dos bons métodos para resolver problemas de \ac{PL}.

Associado a todo problema \ac{PL}, escrito na forma padrão, existe um outro \ac{PL}, chamado \emph{problema dual} dado por
\begin{equation*}
\begin{array}{lc}
\displaystyle \max & b^Ty \\
\text{sujeito a} &\begin{cases} A^Ty \leq c \\
				 y \text{ livre}	
				 \end{cases}\
\end{array}
\end{equation*}
em que $A$, $b$ e $c$ são os mesmos do problema original, que, neste caso, é chamado \emph{problema primal}.
A variável $y\in\Real^{m}$ é chamada variável dual. A fim de que tenhamos restrições de igualdade na definição do problema dual, introduzimos  variáveis de folga duais representadas por $z\in\Real^{n}$, tais que $z\geq 0$, obtendo 
\begin{equation}
\begin{array}{lc}
\displaystyle \max & b^Ty \\
\text{sujeito a} &\begin{cases} A^Ty +z =  c \\
				 z\geq 0, \quad y \text{ livre}	
				 \end{cases}\
\end{array}.
\label{eq:introPL-dual}
\end{equation}


Os problemas primal e dual são comumente chamados par primal-dual e as relações existentes entre estes permitiram a criação ou refinamento de diversos métodos para a solução de \ac{PL}~\cite{Bazaraa:2009uu}. No Capítulo~\ref{chap:mpis} faremos uma explanação mais detalhada sobre esses dois problemas, já que estudaremos métodos primais-duais.

\section{Método simplex}


O Método Simplex foi apresentado por
\textcite{Dantzig:Maximization-of-a-linear:1951y} em 1947 e foi desenvolvido ao
mesmo tempo em que houve a percepção do poder da \ac{PL} como ferramenta auxiliar na
tomada de decisões. Seu pioneirismo deve-se à busca da solução do \ac{PL}
caminhando pelos vértices de maneira a utilizar o valor da função objetivo para determinar qual o próximo vértice adjacente deve ser
visitado, fazendo assim uma aplicação do Teorema~\ref{teo:fundamental-pl}.

Mais que isso, dado um método de escolha do próximo vértice, o conjunto de
vértices possíveis decresce em cada iteração,  ainda no caso não degenerado.
Degenerescência (primal) ocorre quando um vértice em $\Real^n$ é definido por
$p>n$ restrições, quando temos restrições na forma $Ax\leq b$, e um passo de tamanho zero pode ser produzido pelo método. Desta
maneira, o método simplex não sairia do lugar, logo nenhuma melhora na função
objetivo seria alcançada. 


O método simplex se mostrou, e ainda se mostra, robusto e eficiente na solução de grande
quantidade de problemas, e por isso mesmo foi considerado \emph{o método
preferencial} para se resolver \ac{PL}. Entretanto, dado o número exponencial
de  vértices possíveis, havia sempre o receio de que seria necessário esforço
exponencial, isto é, que   todos os vértices da região viável fossem
visitados para que se encontrasse a solução ótima. De fato, \textcite{Klee:1972wi}
foram os primeiros a apresentar uma classe de exemplos patológicos para a qual esse
comportamento  do método simplex apareceu. No exemplo criado por estes
autores, com $n$ variáveis e $2n$ restrições, o método simplex de Dantzig 
precisou passar por todos os $2^n-1$ pontos extremos da região viável antes de
encontrar a solução ótima. 


Apesar disso, nenhum caso com número exponencial de iterações foi encontrado na
vida real, e geralmente apenas uma pequena porcentagem de vértices é
consultada até que se chegue a uma solução ótima. Mais que isso, em geral, o método
simplex mostra comportamento polinomial, sendo linear em relação a $m$ e
sublinear em relação a $n$~\cite[pg.~94]{Fang:1993wu}. Um resumo sobre a
eficiência do método simplex na prática pode ser encontrada no artigo
de~\textcite{Shamir:1987th}.
 

\section{Método de elipsoide}

Enquanto na prática o método Simplex continuava como o método mais robusto e
eficiente, a busca por um método que tivesse \emph{complexidade polinomial} no pior
caso foi satisfeita por \textcite{Khachiyan:A-polynomial-algorithm:1979y} através
de seu  \emph{Método Elipsoide}. Antes de continuarmos, vale a penas fazermos uma observação sobre o sentido que daremos a complexidade.

\begin{obs}
Neste trabalho, quando mencionarmos \emph{complexidade}, o faremos no sentido de um limitante para o número de iterações apenas, descartando aqui o esforço gasto para calcular cada iteração.
Para isso, utilizaremos a seguinte notação: se $\al$ tem ordem de complexidade $n$, indicaremos $\al = \Oset(n)$ significando que  existe constante $\be$ tal que $\al \leq \be n$.
\end{obs}

O  método elipsoide, ao contrário do método simplex,  não se baseia em caminhar pelos
vértices da região factível, mas em diminuir o volume de um elipsoide que
contenha um potencial ponto de solução através de um conjunto de desigualdades
lineares estritas.

Inquestionavelmente, o método elipsoide representou um grande avanço teórico,
significando que a \acl{PL} faz parte dos problemas polinomiais, isto é, que
pode ser resolvido por um algoritmo de complexidade polinomial, já que se provou
que este método tem $\mathcal{O}(n^2(1/\eps))$ iterações, em
que $\eps$ é a precisão desejada para o algoritmo.

Todavia, do ponto de vista prático, o método elipsoide não conseguiu competir com o
método simplex~\cite{Bland:1981vn}, pois sua convergência era muito lenta, na
presença de erros de arredondamento perdia robustez e em cada iteração a
quantidade de memória necessária para armazenamento era muito grande, tornando a iteração cara.
Consequentemente, embora desafortunadamente o método simplex tenha complexidade
exponencial na análise do \emph{pior caso}, experimentos numéricos indicaram que
na prática este era absolutamente superior ao método elipsoide.


\section{Métodos de pontos interiores}



Em problemas práticos, o método Simplex reinou absoluto na solução de problemas
de \ac{PL} até meados da década de 1980 como único método viável para resolver
tal classe de problemas. Em 1984, \textcite{Karmarkar:1984cp} inicia uma nova
abordagem que ficou conhecida como
\acf{MPI} . Por outro lado,  em 1967,
\textcite{DIKIN:InterativeSol1967} já havia publicado um trabalho, no qual  
 um método de pontos interiores é apresentado. O método de Dikin ficou
conhecido como método afim-escala.
      
% De fato, uma busca histórica mostra que em 1955  o primeiro \emph{\acl{MPI}} é
% atribuído a \textcite{Frisch:The-logarithmic-potential:1955t}.
% Este trabalho implementa uma função barreira-logarítmica para encontrar o mínimo local de uma
% função \emph{não-linear}, sujeita a restrições de desigualdade. O método foi
% extensamente estudado por \textcite{Fiacco:Nonlinear-programming::1968y}, porém
% caiu em desuso por conta de sua ineficiência bem como pela presença de
% competidores muito fortes como a programação quadrática sequencial.



A ideia principal dos \ac{MPI} difere-se fundamentalmente da que inspira o
método simplex. No método simplex as soluções encontradas em cada iteração estão
na fronteira da região factível, já que o método visita os vértices do poliedro
que define tal região. Por outro lado, nos \ac{MPI} estas soluções em cada
iteração estão no interior de uma região. 

Isto é feito criando-se uma família
parametrizada de soluções que convergem assintoticamente para uma solução exata,
isto é, temos  um método homotópico. Com efeito, um \ac{MPI} basicamente utiliza um método do tipo Newton nas condições de otimimalidade de Primeira Ordem, ou Condições KKT, do Problema de Programação Linear. Há, nos diferentes métodos, variação  do lado direito que se usa na solução da linearização das condições KKT que o Método de Newton utiliza, podendo gerar, nesse caso,  direções de Newton diferentes. Daí o nome \emph{Métodos} de Pontos Interiores. 

Em resumo,  uma abordagem não-linear é utilizada para resolver um problema
linear, escapando então da  dificuldade que a dimensão do problema apresenta ao
 lidar com as características combinatoriais  do \ac{PL}.


Além disso,  como exposto acima,  no método simplex a quantidade de
iterações pode crescer exponencialmente, de acordo com o tamanho do problema,  o que não se repete
nos \ac{MPI}. Do ponto de vista
da complexidade computacional, esse método também é polinomial,  com
$\mathcal{O}(n(1/\eps))$ iterações~\cite{Karmarkar:1984cp}.
 
  
Ao contrário do método elipsoide, o método de \citeauthor{Karmarkar:1984cp} tem um desempenho muito
melhor e compete com o simplex, sendo consideravelmente melhor  que este em
problemas de grande porte. Uma variante do método de \citeauthor{Karmarkar:1984cp} foi implementada
por~\textcite{Adler:1989fw} e, desde então, a compreensão do funcionamento desses
métodos aumentou consideravelmente, ao mesmo tempo em que variantes têm
sido propostas, muitas delas se apresentando como alternativas computacionais
viáveis ao método simplex.

Atualmente, o melhor algoritmo de pontos interiores conhecido para \ac{PL},  do ponto de vista teórico, 
encontra uma solução com $\eps$-precisão em $\Oset(\sqrt{n}\ln(1/\eps))$
iterações~\cite{Renegar:1988cr}.
De acordo com a teoria geral~\cite[Capítulo 4]{Nesterov:2003wi}, o termo
$\sqrt{n}$  é o melhor que se pode esperar para um \ac{MPI} usando uma função barreira como a
logarítmica. Na prática, no entanto, as implementações de \ac{MPI}  têm desempenho muito melhor que
isso, e, em algumas dessas, o número de iterações é quase constante, independente da dimensão do
problema~\cite{Colombo:2008ia}.

Em linhas gerais, há classes de problemas que são melhor resolvidos
pelo método simplex e outras para os quais os métodos de pontos interiores são mais
adequados. Tamanho e estrutura de esparsidade, entre outros, são fatores
preponderantes para a escolha do  método apropriado. Entretanto, pode-se dizer
que com o aumento da dimensão do problema, os \ac{MPI} ficam mais atraentes e
efetivos. Em alguns problemas altamente esparsos e com  estrutura particular, é possível uma revisão
do método simplex que o torna muito eficaz~\cite{Hall:2005vw}. Para problemas
de fluxo em rede, nos quais uma especialização do método simplex permite
explorar a estrutura do problema de maneira muito
eficiente~\cite{Nemhauser:Integer-and-combinatorial:1988s}, também é comprovada a eficácia do simplex. Vale a pena notar no entanto que, para um problema de fluxo de rede com  cerca de milhões de variáveis, 
\textcite{Resende:1993hh} mostraram que um \ac{MPI} foi mais eficiente.



  




\section{Formulação com variáveis canalizadas e extensões}

Embora a formulação padrão do problema primal de \ac{PL} dada em \eqref{eq:introPL-primal} considere apenas que devamos ter $x$ não-negativo, em problemas práticos, nem sempre isso acontece. 

Podemos ter, por exemplo, alguma componente de $x$ -- ou todo o vetor -- irrestrita ou livre. Para resolver esse problema, considere que $x_{i}$ seja livre, para algum $i$. As variáveis $x_{i}^{+},x_{i}^{-}\geq 0$ são criadas, tais que $x_{i} = x_{i}^{+} -  x_{i}^{-}$. Nesse caso,  uma variável livre é trocada por duas variáveis não-negativas nas restrições do problema.

Pode haver também algum $x_{i}$ com limitante inferior não nulo, digamos, $\ell\in\Real$. Isto significa que se está exigindo agora que $ x_{i}\geq \ell$. Uma forma de  contornar esse problema é  criar a variável $\xtil_{i} = x_{i} - \ell$ e substituir $x_{i}$ por $\xtil +\ell $ nas restrições do problema. Temos, nesse caso,  $\xtil_{i} \geq 0$.


Um caso não menos comum e bastante importante é o da variável $x$ ser canalizada, isto é, possuir algum limitante superior. Queremos dizer com isso que para algum $i$, existe  $u_i>0$ tal que $0\leq x_{i} \leq u_{i}$. Para esse caso, uma saída possível é a seguinte. 

Suponha que existam $p\leq n$ componentes de $x$ que sejam canalizadas. Definimos então a matriz de posto completo $E\in\Real^{p\times n}$, em que cada coluna e cada linha tem exatamente um elemento não nulo e igual a 1. Aqui, $E$  representa a transformação linear $E:\Real^{n}\To \Real^{p}$ que mapeia $x$ em sua parte canalizada $Ex$. Neste caso teremos $Ex \leq u$, em que $u\in\Real^{p}$ é o vetor das canalizações do problema.


Com isso, o \ac{PL} é escrito como 
\begin{equation*}
	\begin{array}{lc}
\displaystyle \min_{x} & c^Tx \\
\text{sujeito a} &\begin{cases} Ax = b \\
								Ex \leq u	\\				 
				 x \geq 0	
				 \end{cases}
\end{array}.
\end{equation*}
Vamos fazer com que a desigualdade desapareça das restrições do problema acima, definindo variáveis de folga primais, dadas pelo vetor $s\in\Real^{p}$, tais que $s\geq 0$ e $Ex +s =  u$. Assim, obtemos o \emph{problema primal canalizado}
\begin{equation}
\label{eq:introPL-primal-bounded}
	\begin{array}{lc}
\displaystyle \min_{x} & c^Tx \\
\text{sujeito a} &\begin{cases} Ax = b \\
								Ex +s =  u	\\				 
				 (x,s) \geq 0	
				 \end{cases}
\end{array}.
\end{equation}

A esse problema, associamos o \emph{problema dual canalizado}
\begin{equation}
\label{eq:introPL-dual-bounded}
\begin{array}{lc}
\displaystyle \max_{(y,v)} & b^Ty + u^{T}v \\
\text{sujeito a} &\begin{cases} A^Ty +z + E^Tv =  c \\
								v + w = 0 \\ 
				 (w,z)\geq 0, \quad y,v \text{ livres}	
				 \end{cases}\
\end{array}.
\end{equation}


É possível escrever tais problemas em uma forma matricial como 
\begin{equation*}
	\begin{array}{lc}
\displaystyle \min_{x} & c^Tx \\
\text{sujeito a} &\begin{cases} \bbm A & 0 \\ E & I_{p}\ebm \bbm x\\ s\ebm = \bbm b\\ u\ebm \\			 
				 (x, s) \geq 0	
				 \end{cases}
\end{array}
\quad \text{e} \quad \begin{array}{lc}
\displaystyle \max_{y,v} & b^Ty + u^{T}v \\
\text{sujeito a} &\begin{cases}  \bbm A^{T} & E^{T} \\ 0 & I_{p}\ebm \bbm y\\ v\ebm +\bbm z\\ w\ebm   = \bbm c\\ 0\ebm \\ 
				 (w,z)\geq 0, \quad y,v \text{ livre}	
				 \end{cases}
\end{array}.
\end{equation*}


Sejam $\Atil = \bbm A & 0 \\ E & I_{p}\ebm$,  $\btil = \bbm b\\ u\ebm$, $\ctil = \bbm c\\ 0\ebm$, $\xtil = \bbm x\\ s\ebm$, $\ytil = \bbm y\\ v\ebm$ e $\ztil = \bbm z\\ w\ebm$. Essas definições transformam \eqref{eq:introPL-primal-bounded} e \eqref{eq:introPL-dual-bounded} em
\begin{equation} \label{eq:introPL-primal-dual-bounded-tilde}
	\begin{array}{lc}
\displaystyle \min_{x} & \ctil^T\xtil \\
\text{sujeito a} &\begin{cases} \Atil\xtil = \btil \\
				 \xtil \geq 0	
				 \end{cases}
\end{array} \quad \text{e} \quad \begin{array}{lc}
\displaystyle \max & \btil^T\ytil \\
\text{sujeito a} &\begin{cases} \Atil^T\ytil +\ztil =  \ctil \\
				 \ztil\geq 0 \quad\ytil \text{ livre}	
				 \end{cases}
\end{array}.
\end{equation}

Observe que essa notação transforma o par primal-dual canalizado na forma padrão do par primal-dual padrão. Assim, temos uma vantagem que utilizaremos em todo este trabalho. Isso é feito porque, agora é possível tratar apenas do par primal dual na forma padrão sem canalizações, e a notação acima indica que todos os resultados obtidos também serão estendidos para os problemas canalizados. Do ponto de vista da implementação, essa abordagem também não causará problemas, já que em nossa implementação, a estrutura de dados é  construída pra lidar com as canalizações sem que a ordem de operações em álgebra linear seja afetada.

Com isso, a partir de agora, consideraremos apenas \ac{PL} na forma dada em \eqref{eq:introPL-primal} e \eqref{eq:introPL-dual} e faremos comentários relativos a canalizações quando oportunos, em particular quando tratarmos da implementação.


Conforme dissemos, a presente tese trata de \ac{MPI} para problemas de \ac{PL}. No entretanto, tais
métodos também podem ser estendidos com o objetivo de resolver problemas não-lineares
de certos tipos, em geral quando a função objetivo é não linear, porém convexa, mas as restrições são lineares e há a exigência da não negatividade das variáveis. 

Em particular, problemas de \acrodef{QP}{Programação Quadrática} \ac{QP} convexa
são usualmente resolvidos com \ac{MPI}, principalmente
quando são de grande porte e é possível especializar o método~\cite{Gondzio:2010bh}. Com efeito, um \ac{QP} convexo pode ser  escrito como
\begin{equation*}
	\begin{array}{lc}
\displaystyle \min_{x} & x^TQx + c^Tx \\
\text{sujeito a} &\begin{cases} Ax = b \\
				 x \geq 0	
				 \end{cases}
\end{array},
\label{eq:QP}
\end{equation*}
em que $Q\in\Real^{n\times n}$ é simétrica semi-definida positiva, $A\in \Real^{m\times
n}$ e $c$, $x$ e $b$ têm dimensão compatível. Com exceção da matriz $Q$, temos a
mesma estrutura de um \ac{PL}.


Neste capítulo, fizemos uma breve introdução à \acf{PL}. No próximo capítulo mencionaremos resultados importantes de PL bem como faremos uma revisão dos Métodos de Pontos Interiores  para \ac{PL}.