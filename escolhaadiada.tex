%!TEX root = tese.tex
\setchapterpreamble[u]{% 
\dictum[Alfred North Whitehead]{``Every really new idea looks crazy at
first.''}}






	\chapter{Método de Escolha Otimizada de Parâmetros em \ac{MPI}}
\label{chap:merit-function}


Este capítulo tem o objetivo de apresentar a principal contribuição deste trabalho, que é o desenvolvimento de uma função de mérito polinomial,  utilizada em cada passo do \acl{MPI} proposto,  que combina três direções -- a direção afim-escala, a direção de centralização e a direção de correção. Além disso, escreve-se a vizinhança $\Nset_{-\infty}$, ajustada para pontos infactíveis, em termos das mesmas variáveis do polinômio. Isso porque a escolha do peso de cada uma das direções,  na direção final é feita através da  otimização global da função de mérito sujeita a um conjunto de restrições que garantem que o próximo ponto continuará na vizinhança. Os pesos das direções, são em geral parâmetros que são escolhidos  \emph{a priori}, nos diversos \ac{MPI}.  Em nosso caso, a escolha será determinada em cada passo do algoritmo, de maneira otimizada.  Esses procedimentos reunidos produzem o que chamamos de   método de escolha otimizada de parâmetros. Além disso, como veremos a seguir,  nosso método é uma generalização de alguns dos \ac{MPI} encontrados na literatura. 

\section{Um KKT escalado}


 
A fim de construir a função de mérito e suas restrições como polinômios, são necessárias algumas transformações, as quais explanamos a seguir. Considere novamente o par primal-dual  (\ref{eq:primal}-\ref{eq:dual}) na sua forma padrão,
i.e.,  

 \begin{equation*}
	\begin{array}{lc}
\displaystyle \min_{x} & c^Tx \\
\text{sujeito a} &\begin{cases} Ax = b \\
				 x \geq 0	
				 \end{cases}\
\end{array}\tag{$P$}
\end{equation*}
e 
 \begin{equation}
	\begin{array}{lc}
\displaystyle \max_{(y,z)} & b^Ty \\
\text{sujeito a} &\begin{cases} A^Ty + z = b \\
				 z \geq 0, \:y \text{ livre}	
				 \end{cases}\
\end{array}.
\tag{$D$}
\end{equation}

Como visto no Capítulo \ref{chap:mpis}, as condições \ac{KKT}  para este
problema são

\begin{subnumcases}{\label{eq:KKT-choice}}
Ax =b,\label{eq:KKT-fac-primal-choice}\\
A^Ty + z =c, \label{eq:KKT-fac-dual-choice}\\
XZe =0,  \label{eq:KKT-complementar-choice}\\
(x,z) \geq 0. \label{eq:KKT-nao-negativ-choice}
\end{subnumcases}
Com isso, dado qualquer $w = (x,y,z)$, os vetores dos resíduos
de \eqref{eq:KKT-choice}, $r_P, r_D$ e $r_C$, podem ser definidos como
\begin{subequations}
\label{eq:KKT-residuals}
\begin{align} 
r_P & =Ax-b,\label{eq:KKT-residuals-a}\\ 
r_D & =A^Ty + z - c,\label{eq:KKT-residuals-b}\\
r_C  &=XZe. \label{eq:KKT-residuals-c}
\end{align} 
\end{subequations}



Seja 
$(x^0,y^0,z^0)$ um ponto inicial que pertença a $\Qset^+$, isto é, um ponto inicial com as componentes de $x$ e de $z$ positivas. Então
\begin{align*} 
r^0_P & =Ax^0-b,\\ 
r^0_D & =A^Ty^0 + z^0 - c, \\
r^0_C  &=X^0Z^0 e > 0. 
\end{align*}
 
% No método proposto, precisa-se garantir que ambos $r^0_P$ e $r^0_D$ sejam
% não-negativos e que seus componentes sejam comparáveis
% -- os motivos para isso ficarão mais claros abaixo.
% Para esse fim, sejam   $H_P$ e $H_D$ matrizes diagonais, tais que cada entrada de
% suas diagonais é formada segundo as seguintes regras:
% \[ (H_P)_i = \begin{cases} \xi_P,& \text{if } (r^0_P)_i \geq0\\
% -\xi_P,& \text{if } (r^0_P)_i <0 \end{cases}, \] para $i=1,\ldots,m$, e \[
% (H_D)_j = \begin{cases} \xi_D,& \text{if } (r^0_D)_j \geq 0\\
% -\xi_D,& \text{if } (r^0_D)_j <0 \end{cases}, \] para $j=1,\ldots,n$,  em que 
% 
% 
% 
% \begin{align}
% \xi_P &=
% \frac{\sqrt{m}}{1+\norm{b}} \\
% \xi_D &=
% \frac{\sqrt{n}}{1+\norm{c}}
% \end{align}
% 

No método proposto, vamos garantir que os resíduos primais e duais iniciais
sejam  não-negativos -- veja Observação \ref{obs:Choice-of-merit-function}.
Escolher um $(x^0,y^0,z^0)$ que gere resíduos iniciais desse tipo pode ser
uma tarefa difícil. Assim, a estratégia adotada é definir  $H_P$ e $H_D$,
matrizes diagonais, tais que cada entrada de suas diagonais é formada segundo as
seguintes regras:
\begin{subequations}
\label{eq:defining_matrices_H}
\begin{equation}
\label{eq:H_P}
 (H_P)_i = \begin{cases} 1,& \text{if } (r^0_P)_i \geq0\\
-1,& \text{if } (r^0_P)_i <0 \end{cases}, 
\end{equation} para $i=1,\ldots,m$, e \begin{equation}
\label{eq:H_D}
(H_D)_j = \begin{cases} 1,& \text{if } (r^0_D)_j \geq 0\\
-1,& \text{if } (r^0_D)_j <0 \end{cases}, 
\end{equation}
 para $j=1,\ldots,n$.
\end{subequations}



Mais que isso, como  $H_D$ e $H_P$ são matrizes de posto completo, o conjunto
solução de  \eqref{eq:KKT-choice} e do sistema 

\begin{subnumcases}{\label{eq:ScaledKKT}}
H_P(Ax-b) =0,\label{eq:ScaledKKT-fac-primal}\\ 
H_D(A^Ty + z -c) =0, \label{eq:ScaledKKT-fac-dual}\\
XZe =0,  \label{eq:ScaledKKT-complementar}\\
(x,z) \geq 0, \label{eq:ScaledKKT-nao-negativ} 
\end{subnumcases}
é o mesmo. De fato, note que apenas se está multiplicando cada linha do sistema
\ac{KKT} original por um escalar. Além disso, garante-se que
\[H_P(r^0_P)\geq 0 \text{ e }H_D(r^0_C)\geq
0.\]
Tal era o objetivo principal dessas transformações.

A otimalidade do sistema \eqref{eq:KKT-choice} -- bem como de
\eqref{eq:ScaledKKT}  -- é alcançada quando todos os resíduos são nulos, ou
melhor dizendo, sempre que esses resíduos forem menores ou iguais a uma  
tolerância pré-estabelecida. Assim propõe-se um método para resolver o sistema
\ac{KKT} escalado que resolva aproximadamente, em cada iteração, para
$(x,y,z)\in\Qset^+$ e algum $\mu>0$, o sistema

\begin{subnumcases}{\label{eq:Homotopy}}
H_P(Ax-b) = 0, \label{eq:Homotopy-fac-primal}\\ 
H_D(A^Ty + z -c) =0, \label{eq:Homotopy-fac-dual}\\
XZe =\mu e,  \label{eq:Homotopy-complementar}\\
(x,z) > 0. \label{eq:Homotopy-nao-negativ} 
\end{subnumcases}


\begin{obs}[Notação]
De acordo com o contexto, considerando-se que $k$ é a iteração atual, pode-se abster-se de indicar a iteração de um vetor ou variável qualquer $t$, isto é, $t^k = t$. Neste caso,  utiliza-se do símbolo circunflexo  \^{} sobre um vetor ou variável $t$ qualquer  em questão para indicar seu valor na iteração $k+1$, isto é, $\hat{t} = t^{k+1}$. 
% Assim $\rho^{k+1}$ será  $\hat{\rho}$ ou ainda $\hat{\rho}(\al,\mu,\sig)$ já que, como será possível ver a partir do Teorema \ref{thm:next=residual}, a escolha de $(\al,\mu,\sig)$ determinará o próximo resíduo. 

\end{obs}

\section{Direções de busca}
\subsection{Direção Afim-escala}\label{sec:affine-scaling-directions}
 

Como consequência da Seção \ref{sec:affine-scalling}, a direção afim-escala
$(\dew)^k$, que resolve aproximadamente  o sistema \eqref{eq:KKT-choice}
é encontrada através na solução do sistema não linear
\begin{subnumcases}{\label{eq:affine-scaling-system}}
A(\dex)^k + r_P^k = 0, \label{eq:affine-scaling-system-primal} \\
A^T(\dey)^k +(\dez)^k + r_D^k =  0, \label{eq:affine-scaling-system-dual}\\
Z^k(\dex)^k + X^k(\dez)^k +  r_C^k = 0. \label{eq:affine-scaling-system-compl}
\end{subnumcases}

Deixando de utilizar o índice da iteração, para simplificar, pode-se encontrar, por meio  da equação \eqref{eq:affine-scaling-system-compl}, o valor de  $\dez$ como
\begin{equation}
\label{eq:de_z}
\dez = -X^{-1}(Z\dex + r_C).
\end{equation}


Se  esta equação for usada e, além disso, $\dez$ for substituído na equação
\eqref{eq:affine-scaling-system-dual}, obtém-se o  sistema
\[
\begin{cases}
A\dex + r_P = 0  \\
A^T\dey -X^{-1}Z\dex -X^{-1}r_C + r_D =  0
\end{cases}.
\]
Este sistema é chamado \emph{Sistema Aumentado} e tem sua forma matricial dada
por
\begin{equation*}
\label{eq:Augmented-system}
\bbm  - D^{-1} & A^T \\
A & 0 
\ebm
\bbm \dex \\ \dey
\ebm = 
\bbm - r_D +X^{-1}r_C  \\  -r_P 
\ebm,
\end{equation*}
em que $D = XZ^{-1}$ é não-singular pois $(x,z)>0$. Mais que isso,
$\dex$ pode ser encontrado como 
\begin{align}
\dex& = -D \left( - r_D +X^{-1}r_C - A^T\dey \right) \notag\\
	 &= D\left(r_D -X^{-1}r_C +A^T\dey \right) \notag\\
	 &= D\left(A^T\dey - t\right), \label{eq:de_x}
\end{align}
em que 
\begin{align*}
t &=  X^{-1}r_C - r_D \\
  &= X^{-1}XZe - A^Ty - z + c \\
  &= z - A^Ty - z + c\\
   &= -(A^Ty - c).
\end{align*}

Usando a fórmula para   $\dex$, encontra-se $\dey$ e neste caso a equação
\eqref{eq:affine-scaling-system-compl} torna-se $AD\left(A^T\dey - t\right) +
r_P = 0$ e logo
\begin{equation}
\label{eq:de_y}
\dey = B(ADt - r_P),
\end{equation}
em que \begin{equation}
\label{eq:define-B}
B = (ADA^T)^{-1}
\end{equation} e portanto $B^{-1}$ é sempre simétrica definida positiva.
Consequentemente, para encontrar $\dew$ usa-se uma fatoração de 
Cholesky de $B^{-1}$ e um
\emph{backsolve} -- ou retrosubstituição.
%XXX Como traduzir o Backsolve?

\subsection{A direção ideal}

O método proposto aqui seguirá a estrutura dos  métodos preditores-corretores
(vide Seção \ref{sec:path-following-methods}). Suponha que, dados o ponto $w =
(x,y,z)$ e um parâmetro  $\mu>0$, seja possível  encontrar $\hat{w}$, solução
do sistema
\begin{equation}
	\label{eq:ideal-system}
\begin{cases} 
A\hat{x} -b = 0 \\
A^T\hat{y} +\hat{z} -c = 0 \\
\hat{X}\hat{Z}e = \mu e
\end{cases},
\end{equation}
através de um único
passo \emph{ideal}
 $\Dew = (\Dex,\Dey,\Dez)$, tal que \[ \hat{w} =
w+\Dew.\]

A estratégia adotada neste trabalho para encontrar esse passo ideal é  escrever
$\Dew = \dew + \Decow$, em que  $\dew$ é a direção afim-escala e $\Decow$ é a direção corretora 
\emph{ideal}.

Procedendo com as substituições, na parte linear de \eqref{eq:ideal-system},
tem-se
\[ A(x+\Dex) - b = A(x+\dex + \Decox) - b = \underbrace{(Ax - b) + A\dex}_{=0
\text{ por \eqref{eq:affine-scaling-system-primal}}} + A\Decox = A\Decox \] 
e
\begin{align*}
A^T(y+\Dey) + (z+\Dez) - c &= A^T(y+\dey+\Decoy) + (z+\dez+\Decoz) - c \\
						  &= \underbrace{A^Ty + z - c + A^T\dey+ +\dez}_{=0 
						  \text{ por \eqref{eq:affine-scaling-system-dual}}} +
						   A^T\Decoy +\Decoz \\
						  &=  A^T\Decoy +\Decoz.
\end{align*} 

Por outro lado, para a parte da complementaridade de 
\eqref{eq:ideal-system}, tem-se que
\begin{align*}
\hat{x}\hat{z} &= (x + \Dex)(z+\Dez) = xz + x\Dez + z\Dex + \Dex\Dez \\
                &= \underbrace{xz + x\dez + z\dex}_{=0 \text{ por
                \eqref{eq:affine-scaling-system-compl} }}  + z\Dex^c + x\Dez^c + (\dex + \Dex^c)(\dez + \Dez^c) \\
                &=  x\Dez^c  +z\Dex^c + \Dex^c\Dez^c + \dex\dez 
                + \dex\Dez^c + \dez\Dex^c \\
                &=         x\Dez^c  +z\Dex^c + \Dex\Dez.     
\end{align*}
  
Usando tais simplificações, obtém-se o seguinte sistema não linear
\begin{equation}\begin{cases}
A\Dex^c =  0\\
A^T\Dey^c +\Dez^c =  0\\
X\Dez^c + Z\Dex^c + \DeX\Dez = \mu e 
\label{eq:corrector-nonlinear}
\end{cases}.
\end{equation}
O vetor $\DeX\Dez$ corresponde a uma correção de segunda ordem, nos moldes dos
trabalhos de \textcite{Mehrotra:1992wr,Gondzio:1996uw}.



A proposta deste trabalho é utilizar uma  forma de generalização das correções
de ordem superior usadas por esses autores.
Essa generalização será feita supondo-se que para algum escalar
$\sig$ positivo e limitado a aproximação
\begin{equation}
\label{eq:aproximacao-direcao}
\DeX\Dez \approx \sig \deX\dez
\end{equation} 
seja aceitável. Estamos aqui fazendo um abuso de notação, ao utilizar o símbolo $\approx$. O que queremos dizer é que, estamos usando como hipótese que existe $\sig$ tal que $\norm{\DeX\Dez - \sig \deX\dez}$ seja suficientemente pequena.
%\footnote{Neste caso, a notação $u \approx \sig v$ significa que
%  $u$ é um vetor que pertence ao subespaço gerado por $v$.}

Em particular note
que se  $\sig = 1$ e $\mu = \tau_{\text{af}}^3/\tau$ -- em que $\tau_{\text{af}}$ é dado
na equação \eqref{eq:tau-affine} e $\tau=x^Tz/n$ -- tem-se o método de
\textcite{Mehrotra:1992wr}. Por outro lado, para \textcite{Gondzio:1996uw}, $\mu$ é
escolhido como no método de Mehrotra e $\DeX\Dez$ é várias vezes aproximado por
direções que projetem, componente a componente, a complementaridade na
vizinhança $N_s(\ga)$. Caso $\mu=0$ e $\sig=1$, e o ponto inicial for factível, temos o método de \textcite{Monteiro:1990vn}.
 De fato, a escolha do valor $\mu$ e a utilização de
correções de ordem superior definem os diferentes tipos de \ac{MPI} que têm sido
utilizados atualmente~\cite{Wright:Primal-dual-interior-point:1997h}. Isso significa que de certa forma, nosso método generaliza alguns dos mais conhecidos \ac{MPI} existentes.




\subsection{Combinando direções}


%XXX:Escrever sobre o trabalho do Fernando


Alguns autores 
combinam direções de correção utilizando pesos para essas direções.
 \textcite{Colombo:2008ia} generalizam e estendem o trabalho de \textcite{Gondzio:1996uw},
fazendo com que a combinação das correções múltiplas  tenha um peso, que é
escolhido fazendo-se uma busca linear. Já \textcite{Jarre:1999tl}  propõem um subproblema
linear que é resolvido a cada iteração e cuja solução determina os pesos que as direções de 
correções de ordem superior terão na direção final. 

\textcite{VillasBoas:2003tg}, também no contexto primal-dual, estudam algumas
vantagens de adiar a escolha do parâmetro de barreira e do tamanho do passo.
Estes autores mostram que o próximo iterado pode ser expresso por uma função
quadrática do parâmetro de barreira, bem como constatam que tal parametrização é útil
para garantir tanto a não negatividade do próximo iterado quanto a proximidade destes da
trajetória central. 

Uma das principais contribuições do nosso trabalho, através da extensão das ideias
de \textcite{VillasBoas:2003tg}, é a escolha do valor de $\mu$ -- que aqui faz as
vezes de parâmetro que determina a trajetória central -- e de $\sig$ -- que vai pesar a contribuição da correção --  sendo feita de maneira adiada, e
por isso tais parâmetros, bem como o tamanho do passo $\al$, estão sendo
tratados como \emph{variáveis}. Para a escolha desses parâmetros, far-se-á uso de uma
função de mérito, a qual será construída a seguir.
 
Primeiramente, note que a aproximação dada em  \eqref{eq:aproximacao-direcao}
transforma o sistema não linear \eqref{eq:corrector-nonlinear} no sistema linear
\begin{equation}\begin{cases}
\label{eq:linear-sytem-DeXDeZcorrec}
A\Decox = 0 \\
A^T\Decoy +\Decoz = 0 \\
X\Decoz + Z\Decox +  = \mu e - \sig\deX\dez
\end{cases}.
\end{equation}
Expressando tal sistema  na  forma
matricial tem-se
\begin{equation}
\label{eq:corrector-linear-matrix}
\bbm A & 0 & 0 \\
0 & A^T & I\\
Z & 0 & X \ebm
\bbm \Decox \\ \Decoy \\ \Decoz
\ebm = 
\bbm 0  \\ 0 \\ \mu e - \sig\deX\dez
\ebm.
\end{equation}
As matrizes do lado esquerdo das equações em
\eqref{eq:affine-scaling-system} e \eqref{eq:corrector-linear-matrix} são as
mesmas. Com isso, podemos resolver  \eqref{eq:corrector-linear-matrix} 
utilizando a mesma fatoração de Cholesky de $B^{-1}$ -- veja a Equação~\eqref{eq:define-B}. 

De fato, o lado direito de  \eqref{eq:corrector-linear-matrix} pode ser
reescrito como
\begin{equation}
\label{eq:corrector-linear-matrix-rhs}
\mu\bbm 0 \\  0 \\ e
\ebm + \sig \bbm 0  \\ 0 \\ - \deX\dez
\ebm.
\end{equation}
Portanto, para $(\mu,\sig)$ qualquer -- ainda não escolhidos --, é possível
encontrar a direção corretora $\Decow$ como
\begin{equation}
\label{eq:Corrector-spllited}
\Decow = \mu \Dew^{\mu} + \sig
\Dew^{\sig},
\end{equation} 
resolvendo então dois sistemas lineares similares ao utilizado na direção afim (vide  Seção 
\ref{sec:affine-scaling-directions}). 

Com efeito, encontra-se 
$\Dew^\mu$ e $\Dew^\sig$  usando a
mesma abordagem da referida seção, i.e., resolvendo
os sistemas $\nabla F(w)\Dew^\mu = (0,0,e)$ e $\nabla F(w)\Dew^\sig = (0,0,-
\deX\dez)$.


Explicitamente, tem-se 
\begin{equation}
\label{eq:De_C} 
\begin{aligned}
& \Dey^{\mu} = B(ADt^\mu )\\
& \Dex^{\mu} = D(A^T\Dey^{\mu} - t^\mu) \\
& \Dez^{\mu} = -X^{-1}(Z\Dex^{\mu} - e)
\end{aligned} \text{\quad e \quad } \begin{aligned}
& \Dey^{\sig} = B(ADt^\sig)\\
& \Dex^{\sig} = D(A^T\Dey^{\sig} - t^\sig) \\
& \Dez^{\sig} = -X^{-1}(Z\Dex^{\sig} + \deX\dez)
\end{aligned}, 
\end{equation}
em que
\[
t^\mu = - X^{-1}e \text{\quad e \quad} t^\sig = X^{-1}\deX\dez.
\]

Desta forma, o maior esforço computacional por iteração do método proposto consiste
em uma fatoração de Cholesky da matriz $B^{-1}$, seguido de três retrosubstituições ou \emph{backsolves}.
 
 
Define-se o próximo ponto, para cada uma das variáveis, como sendo \[\hat{w} = w
+ \al(\dew + \Decow),\] 
ou ainda, expandindo para cada variável bem como
separando as direções, na forma
\begin{subequations}
\label{eq:next-iterate}
\begin{align}
& \hat{x} = x + \al(\dex + \mu\Dex^\mu + \sig\Dex^\sig ),  \\
& \hat{y} = y + \al(\dey + \mu\Dey^\mu + \sig\Dey^\sig ), \\
% \intertext{e}
& \hat{z} = z + \al(\dez + \mu\Dez^\mu + \sig\Dez^\sig ).
\end{align} 
\end{subequations}

Até o presente momento, a tripla $(\al, \mu,\sig)$, em que $\al$ é o tamanho do
passo, ainda não foi escolhida. Para de fato dar o passo indicado acima, a
abordagem proposta neste trabalho é tratar algebricamente $(\al, \mu,\sig)$ como
uma tripla de variáveis reais, utilizar no máximo três \emph{backsolves} para
escolhê-las e finalmente usar a combinação linear das direções $\dew$,
$\Dew^\mu$ e $\Dew^\sig$, com $\al,\mu$ e $\sig$ determinando
as constantes -- ou pesos --  de tal combinação.




A função de mérito, que será  apresentada em sequência, será formulada a partir
dos resíduos do sistema KKT escalado \eqref{eq:ScaledKKT}. Para tanto, na
próxima Seção será mostrado que é possível prever, a depender de
uma escolha de $(\al, \mu,\sig)$, o próximo resíduo deste sistema.


\section{O próximo resíduo}

\begin{defin} 
\label{def:residual-vector}
Definimos  $\rho$, \emph{vetor de resíduos do sistema} \ac{KKT}  escalado
\eqref{eq:ScaledKKT} para um ponto $(x,y,z)$ como
\begin{equation}
\label{eq:residuals}
\rho(x,y,z) = 
\begin{cases}
\rho_P (x,y,z)= H_P(A{x} -b)  \\
\rho_D(x,y,z)= H_D(A^T{y} +{z} -c) \\
\rho_C(x,y,z)= {X}{Z}e
\end{cases}.
\end{equation}

Além disso, seja  $\rho_L = (\rho_P,\rho_D)^T\in\Real^{m+n}$ o \emph{vetor dos
resíduos da parte linear} do  sistema \ac{KKT} escalado. Definimos o
\emph{vetor dos resíduos na iteração $k$} como $\rho^k$.
Por construção $\rho^0>0$ para $(x^0,y^0,z^0)$.
\end{defin} 



Usando a Definição \ref{def:residual-vector}, dado um ponto  $(x,y,z)$, podemos 
prever o resíduo $\nextrho$, para o próximo ponto $(\nextx,\nexty,\nextz)$, a  depender de uma escolha de 
$(\al,\mu,\sig)$, através do Teorema \ref{thm:next=residual}. 

\begin{teo}
\label{thm:next=residual}
O próximo resíduo para o sistema KKT escalado  \eqref{eq:ScaledKKT} é escrito, em
termos da tripla $(\al,\mu,\sig)$, como 
\begin{equation}
\label{eq:next-residual-all}
\nextrho(\al,\mu,\sig) = \bbm (1-\al)\rho_L \\   (1-\al)\rho_C+ \al\mu + \al(\al-\sig)L_{0,0} +  
\al^2 \La(\mu,\sig) \ebm
% \begin{cases}
% (\hat{\rho}_L)_\ell  = (1-\al)(\rho_L)_\ell ,  \\
% \text{para } \ell= 1,\ldots,n+m. \\
% (\hat{\rho}_C)_j =  (1-\al)(\rho_C)_j+ \al\mu + \al(\al-\sig)(L_{0,0})_j +  
% \al^2 \La(\mu,\sig)_j,
%  \\
%   \text{para } j = 1,\ldots, n.
% \end{cases}
\end{equation}
em que 
\begin{equation}
\label{eq:Lambda-mu-sigma}
\La(\mu,\sig) = \left(\mu^2
 L_{2,0} + \mu L_{1,0} + \mu \sig L_{1,1} + \sig^2L_{0,2} + \sig
 L_{0,1}\right),
\end{equation}
e
\begin{equation}
\label{eq:defining-Lij}
\begin{aligned}
	L_{0,0} & = \dex\dez, & L_{1,1}  & = \Dex^\mu\Dez^\sig +
			\Dex^\sig\Dez^\mu, \\	
	L_{1,0} &  = \dex\Dez^\mu +
			\dez\Dex^\mu,  & L_{0,1}  & = \dex\Dez^\sig + \dez\Dex^\sig, \\	
	L_{2,0} &  = \Dex^\mu\Dez^\mu,  & L_{0,2}  & = \Dex^\sig\Dez^\sig .\\	
\end{aligned}
\end{equation}

\end{teo}

O Teorema \ref{thm:next=residual} é consequência direta da aplicação
dos Lemas \ref{lemma:linear-residual} e \ref{lemma:nonlinear-residual}, que
atacam a parte linear e não linear de  $\hat{\rho}$ de forma separada. Esses lemas
e suas demonstrações encontram-se na sequência. %no Apêndice
% \ref{sec:tech-resultas}. 
 
\begin{lema}\label{lemma:linear-residual}
O resíduo da parte linear de  \eqref{eq:ScaledKKT}, para o próximo iterado é
escrito como

\[
\hat{\rho}_L(\al,\mu,\sig) = (1-\al)\rho_L.
\]
 
\end{lema}

\begin{proof}
Para a equação \eqref{eq:ScaledKKT-fac-primal} temos
\[
\begin{aligned}
\hat{\rho}_P & = H_P(A\hat{x} -b) = H_P(A(x + \al(\dex +
\Decox )) -b) \\ 
			& = H_P(A(x +  \al\dex) - b) +
 \al H_PA\Decox.
\end{aligned}
\]
Como $A\dex = b- Ax$, vale a seguinte igualdade:
\[
\begin{aligned}
H_P(A(x +  \al\dex) - b) & = H_P((1-\al)A\dex) \\
							% & = 	 H_P((\al -1)(b-Ax)) \\
							& =  (1 - \al)\underbrace{H_P(Ax - b)}_{\rho_P} 	\\
							& = (1 - \al)\rho_P.					
\end{aligned}
\]
Por outro lado, pela equação \eqref{eq:corrector-nonlinear}, $A\Dex^c= 0$. Logo
$\hat{\rho}_P = (1-\al)\rho_P$.

A prova para parte dual da factibilidade é similar. Com efeito,
\begin{align}
\hat{\rho}_D & = H_D(A^T\hat{y} + \hat{z} -c)   \notag \\
			& = H_D\left[ A^T\left(y + \al(\dey + \Decoy)\right) + \left(z + \al(\dez + \Decoz )\right)
			-c\right] \notag\\
 			& = H_D\left[A^Ty  + z -c +  \al(A^T\dey + \dez)\right] +
 			\label{eq:next-rho-dual-a} %\\
 			%& \quad + 
 			H_D\underbrace{\left(A^T\Decoy   + \Decoz\right)}_{=0 \text{ por
 			\eqref{eq:corrector-nonlinear}}}. %\notag 
\end{align}


Como $A^T\dey + \dez = c - A^Ty - z$, a equação \eqref{eq:next-rho-dual-a}
torna-se
\[
H_D\left[A^Ty  + z -c +  \al(A^T\dey + \dez)\right]  = (1-\al)H_D 
(A^Ty + z - c)  = (1-\al)\rho_D.
\]

Portanto, $ \hat{\rho}_D = (1-\al)\rho_D$.
\end{proof}
 
 
\begin{lema}\label{lemma:nonlinear-residual}
O resíduo da parte da complementaridade de  \eqref{eq:ScaledKKT}, para o próximo iterado é
escrito como
\begin{equation}
\label{eq:next-residual-complementar}
\hat{\rho}_C = (1-\al)\rho_C + \al\mu e+ \al(\al-\sig)L_{0,0} +
\al^2\La(\mu,\sig).
\end{equation}
em que os vetores $L_{i,j}$, $i,j \in \{0,1,2\}$ são definidos por
\eqref{eq:defining-Lij}.
\end{lema}

\begin{proof} 
Para encontrar o resíduo da parte complementar para o próximo iterado,
considere que pode-se escrever
 $\hat{\rho}_C  =
\hat{x}\hat{z}$. Então
\begin{align}
\hat{x}\hat{z} & = \left(x + \al(\dex + \Decox) \right)\left( z + \al(\dez + \Decoz
)\right) \notag \\
& = \al^2(\dex+\Decox)(\dez + \Decoz) +  \al\left[  (x\dez+z\dex) +
(x\Decoz + z\Decox)\right] + xz \label{eq:next-residual-complementar-1}
\end{align}


Note que  $ (x\dez+z\dex) = -xz$ por \eqref{eq:affine-scaling-system}. Além
disso usando a equação \eqref{eq:Corrector-spllited}, claramente  $x\Dez^\mu +
z\Dex^\mu = e$ e $x\Dez^\sig + z\Dex^\sig = - \dex\dez$. Então $(x\Dez^c +
z\Dex^c) = \mu e - \sig\dex\dez$.
 
Assim, a equação \eqref{eq:next-residual-complementar-1} torna-se
\begin{subequations}
\begin{align}
\hat{x}\hat{z} & =  \al^2(\dex + \Dex^c)(\dez + \Dez^c) + \al (-xz + \mu   e
- \sig\dex\dez) + xz \notag\\
& = (1-\al) xz + \al\mu e +
\al(\al-\sig)\dex\dez    +
\label{eq:next-residual-complementar-2a} \\
& \quad + \al^2\left(\dex\Dez^c + \dez\Dex^c + \Dex^c\Dez^c\right).
\label{eq:next-residual-complementar-2b}
\end{align}
\end{subequations}

Como $\Decow = \mu\Dew^\mu + \sig\Dew^\sig$, então
\begin{align*}
\dex\Dez^c &=  \mu\dex\Dez^\mu + \sig\dex\Dez^\sig,\\
\dez\Dex^c &=  \mu\dez\Dex^\mu + \sig\dez\Dex^\sig,\\
\intertext{e}
\Dex^c\Dez^c& =   (\mu\Dey^\mu + \sig\Dey^\sig) (\mu\Dez^\mu
			+ \sig\Dez^\sig) \\ 
			& = \mu^2\Dex^\mu\Dez^\mu + \mu\sig \left(\Dex^\mu\Dez^\sig +
			\Dex^\sig\Dez^\mu \right) + \sig^2\Dex^\sig\Dez^\sig.
\end{align*}
Consequentemente a equação \eqref{eq:next-residual-complementar-2b} pode ser
expressa como
\begin{multline}
\label{eq:next-residual-complmentar-3b}
\al^2\left(\mu^2\Dex^\mu\Dez^\mu + \mu\sig \left(\Dex^\mu\Dez^\sig +
			\Dex^\sig\Dez^\mu \right) \right. +  \\ \left. + \mu\left(\dex\Dez^\mu +
			\dez\Dex^\mu \right) +   \sig\left(\dex\Dez^\sig + \dez\Dex^\sig\right) +
			\sig^2\Dex^\sig\Dez^\sig \right).
\end{multline}


Basta agora somar  \eqref{eq:next-residual-complmentar-3b}  e
 \eqref{eq:next-residual-complementar-2a}. Definindo os vetores
$\La(\mu,\sig)$ como em \eqref{eq:Lambda-mu-sigma} e $L_{i,j}$ como em
\eqref{eq:defining-Lij} e substituindo onde for necessário, finalmente
encontra-se a equação \eqref{eq:next-residual-complementar} levando em conta que $\rho_C = xz$.
\end{proof} 


O vetor  $\hat{\rho}(\al,\mu,\sig) \in \Real^q$, em que  $q = m+2n$ é
precisamente o número de linhas de  \eqref{eq:ScaledKKT}. Além disso, uma
consequência do Teorema \ref{thm:next=residual} é que todos os resíduos
permanecem não-negativos, se nosso ponto inicial é interior. Formalmente temos o seguinte Corolário. 

\begin{corol}\label{cor:positive-residual}
Em cada iteração $k$, para  $ \al \in(0,1]$ e  $(\mu, \sig)>0$,
$\rho^k(\al,\mu,\sig) \geq 0$.
\end{corol} 
\begin{proof}

Para a parte linear de  \eqref{eq:ScaledKKT}, pelo Lema
\ref{lemma:linear-residual}, $\hat{\rho}_L(\al,\mu,\sig) = (1-\al)\rho_L$. Como
garantimos que, $\rho^0\geq 0$, $0\leq\al\leq1 $, por  indução, o corolário
se verifica.

A parte da complementaridade, por construção,  é positiva, já que 
$(x,y,z)\in\Qset^+$. Além disso, faremos a cada iteração o teste da razão, isto é, escolheremos $\al_k$,
tal que 
\begin{equation}
	\label{eq:ratio-test}
\al_k < \tilde{\al}_k = \frac{-1}{\min\left\{
(X^k)^{-1}\Dex^k,(Z^k)^{-1}\Dez^k,-1 \right\}}. 
\end{equation}

Logo, $\rho^k_C>0$ para todo $k$ e o corolário é válido. 
\end{proof}

\section{Um polinômio como  função de mérito}


Em geral, funções de mérito para \ac{MPI} servem para dar uma medida de quão
próximo se está da solução do problema, embora nem sempre elas sejam usadas como
critério de parada do algoritmo. 

\textcite{Zhang:2006ic}, por exemplo, quer escolher o tamanho de passo de seu
algoritmo, tal que a  complementaridade e a factibilidade primal-dual sejam
reduzidas. Para isso, este autor define sua função de mérito como a soma do
\emph{gap} de complementaridade e da norma-2 do resíduo primal-dual, i.e., \[\phi(x,y,z) =
\norm{(r_P,r_D)} + x^Tz\] e escolhe $\al$ tal que essa função seja minimizada.
Nada obstante, essa função é utilizada apenas de maneira teórica, para
demonstrar a convergência do método.

Do ponto de vista prático, o  \texttt{PCx} \cite{Czyzyk:1999hk} utiliza-se de
uma função de mérito para detectar se o problema é infactível ou se a solução é desconhecida
ou sub-ótima. Nesse caso, a função é dada por 
\[
\phi(x,y,z) = \frac{\norm{r_P}}{\max \{1,\norm{b} \} } + \frac{\norm{r_D}}{\max
\{1,\norm{c} \} } + \frac{c^Tx - b^Ty}{\max \{1,\norm{b},\norm{c} \} }.
\]
Note que essa função de mérito tem relação com o próprio critério de parada
do algoritmo, como visto em  \eqref{eq:termination-criteria-pcx}.



Neste trabalho, será definida uma função de mérito que não só sirva como medida
de complementaridade e factibilidade, mas que também sirva de guia para a
escolha do próximo ponto. De maneira similar aos autores acima citados, serão
usadas medidas  da infactibilidade  e do \emph{gap} de complementaridade. 

\begin{defin}[Função de Mérito]
\label{def:merit-function}
Definimos a \emph{função de mérito} de um ponto  $(x,y,z)$ como
\begin{align}
\varphi(x,y,z) & = \frac{1}{m+n}\norm{\rho_L}_1+ 
\frac{x^Tz}{n}, \label{eq:merit-function-as-sum}
\end{align}
em que $\rho_L$ é dado por  \eqref{eq:residuals} no ponto
$(x,y,z)$.

\end{defin}

\begin{obs}
\label{obs:Choice-of-merit-function}
A Definição \ref{def:merit-function} é uma das possíveis maneiras de medir quão
perto da solução está um ponto $(x,y,z)$. Em particular,  esta escolha possui algumas
propriedades que serão exploradas para escolher-se a tripla $(\al,\mu,\sig)$, a
saber:

\begin{enumerate}[(i)] 

	
% FIXME Elaborar melhor a questão da não-diferenciabilidade e da necessidade de 
% utilizar a norma para obter um polinômio 

\item \label{it:obs-Choice-merit-rho-l} A utilização das matrizes $H_P$ e $H_D$  e o Corolário
\ref{cor:positive-residual} garantem que dado $(x,y,z)$ calculado pelo
método e que esteja em $\Qset^+$ ,  o sinal de cada linha de $\rho_{L}$ permanece igual em todas as iterações --  de fato, determinamos que tais linhas sejam não-negativo. Neste caso, temos certeza de que para cada linha $i$ de $\rho_{L}$,  $\rho_L(x,y,z)_{i}\geq 0$. Portanto, podemos descartar o módulo na utilização da norma-1 de $\rho_{L}$, isto é,
\[\frac{1}{m+n}\norm{\rho_L}_1 = \frac{1}{n+m}\sum_{\ell=1}^{n+m}(\rho_L)_\ell. \] 
Tal escolha permite transforma a função de mérito em um polinômio nas variáveis $(\al,\mu,\sig)$, conforme veremos a seguir; 



\item \label{it:obs-Choice-merit-rho-C} Pode-se rescrever o
\emph{gap} de complementaridade como
 \[x^Tz = \sum_{i=1}^{n}x_iz_i = \sum_{i=1}^{n}(\rho_C)_i;\]

\item A média do \emph{gap} de complementaridade, $x^Tz/n$, é usada em geral
como parte determinante do valor para o parâmetro de barreira (vide Seção
\ref{subsec:barrier-problem}). Em uma solução ótima, este valor é
nulo;    
\item Também devido ao Corolário \ref{cor:positive-residual}, dado
$(x,y,z)\in\Qset^+$ gerado pelo método, vale
\[
\varphi(x,y,z)\geq0;
\]
	\item Finalmente, se $(x^*,y^*,z^*)$ é solução de
\eqref{eq:ScaledKKT}, então $\varphi(x^*,y^*,z^*)=0$.
\end{enumerate}
\end{obs}


Por conta dos itens \eqref{it:obs-Choice-merit-rho-l} e \eqref{it:obs-Choice-merit-rho-C} da Observação \ref{obs:Choice-of-merit-function}, podemos rescrever
\eqref{eq:merit-function-as-sum} como


\begin{align}
\varphi(x,y,z) & = \frac{1}{n+m}\sum_{\ell=1}^{n+m}(\rho_L)_\ell+ 
\frac{1}{n}\sum_{i=1}^{n}(\rho_C)_i.\label{eq:merit-function-rho}
\end{align}

Para fins de notação, define-se o \emph{operador média} para qualquer vetor
$v\in\Real^p$ como \[\dbvec{v} = \frac{1}{p}\sum_{i=1}^p v_i.\] Este operador
encontra a média aritmética das componentes do vetor em questão. 

Com esta notação, pode-se representar a função de mérito
\eqref{eq:merit-function-rho} como
\begin{equation}\label{eq:merit-function-as-sum-bar}
\varphi(x,y,z)  = \dbvec{\rho_L}+ 
\dbvec{\rho_C}.
\end{equation}



Ao usarmos a Definição \ref{def:merit-function} em conjunto o Teorema \ref{thm:next=residual}, poderemos encontrar o valor função de mérito para o próximo ponto $(\hat{x},\hat{y},\hat{z})$, denotando-a como $\nextphi$, a depender de uma
escolha de $(\al,\mu,\sig)$. Com efeito, o Teorema \ref{thm:varphi} a seguir nos dá a expressão algébrica de $\nextphi$.

\begin{teo}
\label{thm:varphi}  
A função de mérito para um ponto $(\nextx,\nexty,\nextz)$ da próxima iteração do Algoritmo \ref{alg:optimized-choice-of-parameters} pode ser escrita como o {polinômio} 
$\nextphi:\Real^3\to\Real$ a depender das variáveis $(\al,\mu,\sig)$, com a seguinte
expressão:
\begin{equation}
\label{eq:merit-function-al-mu-sig}
\nextphi(\al,\mu,\sig) = (1-\al)(\dbvec{\rho_L} +
\dbvec{\rho_C}) + \al\mu + \al(\al-\sig)\dbvec{L_{0,0}} +
\al^2\dbvec{\Lambda(\mu,\sig)},
\end{equation}
em que 
\begin{equation}
	\label{eq:Lambda-mu-sig}
	\dbvec{\Lambda(\mu,\sig)} = \mu^2
 \dbvec{L_{2,0}} + \mu \dbvec{L_{1,0}} + 	\mu \sig \dbvec{L_{1,1}} +
 \sig^2 \dbvec{L_{0,2}} + \sig \dbvec{L_{0,1}}.
 \end{equation} 
 
  \end{teo}

\begin{proof}  Por conta da Equação \eqref{eq:next-residual-all}, sabe-se a expressão do
próximo resíduo, a depender de uma escolha de $(\al,\mu,\sig)$. Neste caso, 
a função de mérito preditiva será
\begin{equation}
\label{eq:merit-function}
\nextphi(\al,\mu,\sig) =
\dbvec{\hat{\rho}_L}(\al,\mu,\sig)+
\dbvec{\hat{\rho}_C}(\al,\mu,\sig).
\end{equation}

 Aplicando diretamente o Teorema \ref{thm:next=residual} na
Equação \eqref{eq:merit-function} e utilizando a definição do operador média de vetor, temos que
 \[
\begin{aligned}
\nextphi(\al,\mu,\sig) & =
\frac{1}{m+n}\sum_{\ell=1}^{m+n}(\hat{\rho}_L)_\ell +
\frac{1}{n}\sum_{j=1}^{n}(\hat{\rho}_C)_j \\
						 & = \frac{1}{m+n}\sum_{\ell=1}^{n+m}\left[(1-\al)(\rho_L)_\ell 
						 \right] +
						 \\
				 	 & \quad  + \frac{1}{n}\sum_{j=1}^{n}  \left[ (1-\al)(\rho_C)_j +
				 	 \al\mu + \al(\al-\sig)(L_{0,0})_j + \al^2{\Lambda(\mu,\sig)}_j \right] \\
				 	 & = (1-\al)\left(\dbvec{\rho_L} + \dbvec{\rho_C}\right) + \al\mu +
\al(\al-\sig)\dbvec{L_{0,0}} + \al^2\dbvec{\Lambda(\mu,\sig)},	
\end{aligned}
\]
o que finaliza a demonstração
\end{proof}


Consideramos que a função de mérito para o próximo ponto, $\nextphi(\al,\mu,\sig) $ bem caracteriza todas propriedades da Observação \ref{obs:Choice-of-merit-function}. Com isso, alcança-se o objetivo de encontrar uma função de mérito que tenha boas propriedades matemáticas -- um polinômio de grau máximo 2 em cada uma das variáveis $(\al,\mu,\sig)$ --  e que ao mesmo tempo dê uma medida adequada da qualidade solução.


\begin{obs}\label{obs:L_02-L20}
É fácil ver que   \[\dbvec{L_{2,0}} = \frac{1}{n} \sum_{i=1}^n
\Dex_i^\mu\Dez_i^\mu = \frac{(\Dex^\mu)^T(\Dez^\mu)}{n} = 0\] e que  
  \[\dbvec{L_{0,2}} = \frac{1}{n} \sum_{i=1}^n
\Dex_i^\sig\Dez_i^\sig = \frac{(\Dex^\sig)^T(\Dez^\sig)}{n} = 0.\] 
Com efeito, pelas equações \eqref{eq:corrector-linear-matrix} e \eqref{eq:corrector-linear-matrix-rhs},
 tanto $\Dex^\mu$ e $\Dez^\mu$ quanto $\Dex^\sig$ e $\Dez^\sig$ são vetores ortogonais. 

 %FIXME: Devo fazer o que com essa parte? Posso comentar depois.
% \textcolor{red}{ Nada obstante, na prática ter-se-á $\dbvec{L_{2,0}}  \geq 
% \mathtt{tol}$ e $\dbvec{L_{0,2}}  \geq  \mathtt{tol}$, já que só é possível
% resolver os sistemas
% \eqref{eq:corrector-linear-matrix} através de aritmética de ponto
% flutuante\cite{Golub:1996wp}. Com isso, por enquanto, tais escalares serão
% considerados não nulos.}
% \textcolor{blue}{Posso falar disso depois, na parte de implementação? }

\end{obs}


Definindo os coeficientes \[ \left\{
\begin{aligned}
a_{0,0,0} &= (\dbvec{\rho_L}_k + \dbvec{\rho_C}_k)     & a_{2,1,1} &=
\dbvec{L_{1,1}}\\
a_{1,0,0} &= - (\dbvec{\rho_L}_k + \dbvec{\rho_C}_k)   & a_{2,1,0} &=
\dbvec{L_{1,0}}\\
a_{1,1,0} &= 1 				    & a_{2,2,0} &= \dbvec{L_{2,0}}=0\\
a_{1,0,1} &= -  \dbvec{L_{0,0}} & a_{2,0,1} &= \dbvec{L_{0,1}}\\
a_{2,0,0} &=  \dbvec{L_{0,0}} & a_{2,0,2} &= \dbvec{L_{0,2}}=0\\
\end{aligned}\right.	
\]
e adotando a convenção de que se o coeficiente  $a_{\ell,p,s}$ não está definido
acima, então será nulo, pode-se escrever $\nextphi$ como
\[
\nextphi(\al,\mu,\sig) = \sum_{\ell= 0}^2\sum_{p=0}^2 
\sum_{s=0}^2a_{\ell,p,s}\al^\ell\mu^p\sig^s,
\]   
ou explicitamente
\begin{equation}
\begin{aligned}
\nextphi(\al,\mu,\si) & = 	a_{0,0,0} + a_{1,0,0}\al + a_{1,1,0}\al\mu + 
a_{1,0,1}\al\sig  
\\
&\quad + a_{2,0,0}\al^2
 + a_{2,1,0}\al^2\mu   + a_{2,0,1}\al^2\sig\\ 
% & \quad 	 \textcolor{red}{+ a_{2,2,0}\al^2\mu^2  + 	a_{2,0,2}\al^2\sig^2}\\ 
& \quad + a_{2,1,1}\al^2\mu\sig.
\end{aligned}
\label{eq:varphi-poly-form}
\end{equation}

%XXX: Usar 8 ou 10?
Esses 8 
% \textcolor{red}{(10)} 
coeficientes de três índices  não-nulos são
os únicos coeficientes que são calculados nessa implementação, além das
direções.

% \textcolor{red}{
% Furthermore, we define
% 
% \[
% \begin{aligned}
% \phi_2(\mu,\sig) & =  \mu\left( a_{2,2,0}\mu + a_{2,1,1}\sig +a_{2,1,0} \right)
% + \sig\left( a_{2,0,2}\sig +  a_{2,0,1} 	\right) +  a_{2,0,0}\\
% \phi_1(\mu,\sig) & =  a_{1,0,0} + a_{1,1,0}\mu +
% a_{1,0,1}\sig   \\
% \phi_0(\mu,\sig) & = a_{0,0,0}
% \end{aligned}
% \]
% so that for each fixed pair $(\mu,\sig)$ we have
% \begin{equation}
% \label{eq:next-varphi-alternative}
% \nextphi(\al) = \al(\phi_2\al + \phi_1) + \phi_0.
% \end{equation}}



O método que está sendo proposto nesta tese encontra em cada iteração o
argumento do mínimo global $(\al^*,\mu^*,\sig^*)$ para um polinômio
 $\nextphi(\al,\mu,\sig)$. Como $\nextphi$ é capaz de predizer a média
 aritmética do próximo resíduo, esta escolha ótima permite um passo em uma
 direção que minimizará  o resíduo $\rho$, pelo menos em média. Com isso
 teremos um problema de otimização global a resolver em cada iteração.
 
 
 \section{Vizinhança como restrições polinomiais}
 Na Seção \ref{subsec:neighbouhoods}, mostrou-se que bons \acl{MPI} baseiam-se na
  utilização de vizinhanças que fazem com que os iterados estejam a distâncias
  razoáveis da trajetória central. Nesta seção mostra-se  que, se um
  ponto pertence a um conjunto viável  gerado por funções polinomiais de terceiro grau 
  nas variáveis $(\al,\mu,\sig)$, construídas a
  partir da vizinhança  $\Nset_{-\infty}$, então esse ponto  também pertencerá
  a essa vizinhança.
  Com isso, tais funções serão usadas como restrições do problema de otimização
  global da função de mérito $\nextphi$, garantindo que o próximo
  iterado não só reduza o valor da função de mérito, mas também que o próximo
  ponto tenha as boas propriedades  da  vizinhança $\Nset_{-\infty}$ possui.
  
  
 % FIXME Reorganizar esta seção com vistas no capítulo da convergência.



% Como visto no capítulo anterior, \textcite{Colombo:2008ia}  propuseram  a
% vizinhança simétrica $\Nset_s(\ga)$ dada na Equação \eqref{eq:symmetric-neig} --
% baseada na vizinhança $\Nset_{-\infty}(\ga)$ --  com objetivo de melhorar a
% \emph{centralidade} de um ponto, i.e., como o espalhamento dos produtos
% complementares $x_iz_i$, para $i=1,\ldots,n$. Estes autores entendem que tal
% vizinhança é a pedra angular através da qual suas implementações obtém boas
% performances, já que tais condições impedem que os pares da complementaridade
% torne-se muito grandes ou muito pequenos antes do tempo adequado.


% Como este trabalho  baseia-se em pontos infactíveis mas ao mesmo tempo
% pretende utilizar as boas propriedades da vizinhança  $\Nset_s(\ga)$,
% é necessário estendê-la. No Capítulo \ref{chap:mpis}, página \pageref{eq:infeasible-infty-neig}
% mostrou-se que \textcite{Kojima:1993fe}, assim como 
% \textcite[pg.~110]{Wright:Primal-dual-interior-point:1997h}, utilizaram em seu
% método infactível a vizinhança $\Nset_{-\infty}(\ga,\be)$.


% Assim, usando a Definição \ref{def:residual-vector}, define-se  a vizinhança
% simétrica infactível $\Nset_s(\ga,\be)$, para o sistema KKT escalado
% \eqref{eq:ScaledKKT}, como 
% \begin{equation}
% \label{eq:infeasible-symmetric-neig}
%  \Nset_{s}(\ga,\be) =
%  \left\{(x,y,z) \in \Qset^+:\frac{\norm{\rho_L}}{\norm{\rho^0_L}} \leq
% \beta\frac{\tau}{\tau_0}, \ga\tau\leq x_iz_i \leq
% \frac{1}{\ga}\tau,\forall i =1,\ldots,n  \right\},
% \end{equation}
% em que $\ga\in(0,1)$, $\tau=x^Tz/n$ e $\be\geq1$.



Vamos considerar, antes de mais nada, a relação existente entre o resíduo linear escalado $\rho_L$ na iteração $k$ com o resíduo 
linear da iteração $k+1$, dada no Teorema \ref{thm:next=residual}. É possível, generalizar tal relação usando a  proposição seguinte que trata do resíduo, quanto do resíduo escalado. 
\begin{prop}\label{prop:nu_k}
 Seja $\{(x^k,y^k,z^k)\}$ gerada pelo Algoritmo \ref{alg:optimized-choice-of-parameters}. Então para $k\geq0$, vale  

\begin{equation}
\label{eq:residual-norm-relation-nonscaled}
% \norm{(\rho_P^{k+1},\rho_D^{k+1})}_1  = (1-\al_k) \norm{(\rho_P^{k},\rho_D^{k})}_1 = \nu_{k+1} \norm{(\rho_P^{0},\rho_D^{0})}_1,
\begin{aligned}
 r_P^{k+1}  & = (1-\al_k) r_P^{k} = \nu_{k+1}  r_P^{0}, \\
  r_D^{k+1}  & = (1-\al_k) r_D^{k} = \nu_{k+1}  r_D^{0}, \\
\end{aligned}
\end{equation}
e além disso
\begin{equation}
\label{eq:residual-norm-relation}
% \norm{(\rho_P^{k+1},\rho_D^{k+1})}_1  = (1-\al_k) \norm{(\rho_P^{k},\rho_D^{k})}_1 = \nu_{k+1} \norm{(\rho_P^{0},\rho_D^{0})}_1,
 \rho_L^{k+1}  = (1-\al_k) \rho_L^{k} = \nu_{k+1}  \rho_L^{0},
\end{equation}
em que  $\nu_0 = 1$ e 
\begin{equation}
	\label{eq:nu_k+1}
\nu_{k+1} = (1-\al_k)\nu_k = \prod_{j=0}^{k}(1-\al_j) \geq 0.
\end{equation}
\end{prop}

\begin{proof}
	Pela definição de resíduos  dada em \eqref{eq:KKT-residuals}   e pela Definição \ref{def:residual-vector}, temos que para todo $k$, $\rho_{P}^{k} = H_{P}r_{P}^{k} $ e  $\rho_{D}^{k} = H_{D}r_{D}^{k} $. 

	Como $H_{P}$ e $H_{D}$ são não-singulares, e como $\rho_L^{k} = (\rho_P^{k},\rho_D^{k})$,  da primeira equação de \eqref{eq:next-residual-all} do Teorema \ref{thm:next=residual}, segue que $\rho_P^{k}  = (1-\al_{k-1})\rho_P^{k-1}$ e logo $
			H_{P}r_{P}^{k}  = (1-\al_{k-1})H_{P}r_{P}^{k-1}$ concluindo que  
		$H_{P}^{-1}H_{P}r_{P}^{k}  = (1-\al_{k-1})r_{P}^{k-1}$, isto é, $r_{P}^{k}  = (1-\al_{k-1})r_{P}^{k-1}$.

	Daí, por raciocínio indutivo segue que 
	\[
	\begin{aligned}
		r_{P}^{k} & = (1-\al_{k-1})r_{P}^{k-1} \\
	  			   & = (1-\al_{k-1})\cdots(1-\al_{0})r_{P}^{0}\\
				   & = \nu_k r_{P}^{0}. 
	\end{aligned}
	\]
Similarmente,  $\rho_D^{k}  = (1-\al_{k-1})\rho_D^{k-1}$ e logo $
			H_{D}r_{D}^{k}  = (1-\al_{k-1})H_{D}r_{D}^{k-1}$, o que significa que   $r_{D}^{k}  = (1-\al_{k-1})r_{D}^{k-1}$. Novamente,  por raciocínio indutivo segue que 
	\[
	\begin{aligned}
		r_{D}^{k} & = (1-\al_{k-1})r_{D}^{k-1} \\
	  			   & = (1-\al_{k-1})\cdots(1-\al_{0})r_{D}^{0}\\
				   & = \nu_k r_{D}^{0}. 
	\end{aligned}
	\]

	Finalmente, temos que 
	\begin{multline*}
	\rho_L^{k} = (\rho_P^{k},\rho_D^{k}) = (H_{P}r_P^{k},H_{D}r_D^{k}) =\\ = (H_{P}\nu_k r_{P}^{0},H_{D}\nu_k r_{D}^{0})  
	= \nu_k (H_{P}r_{P}^{0},H_{D} r_{D}^{0}) =\nu_k(\rho_P^{0},\rho_D^{0}) = \nu_k \rho_L^{0},
		\end{multline*}	
		o 	que finaliza a demonstração.
\end{proof}

Como  $\nu_k$ é uma constante não negativa,  uma consequência desta proposição é que se $r_L^0\neq 0 $, o que implica que $\rho_L^0\neq 0 $, para toda norma-p, vale 
\[
\nu_k =\frac{\norm{r_L^{k}}_p}{\norm{r_L^{0}}_p} = \frac{\norm{\rho_L^{k}}_p}{\norm{\rho_L^{0}}_p}.
\]
Logo, pela definição de \emph{operador média}, segue que
\begin{equation}
	\label{eq:nu_k}
	\nu_k = {\dbvec{\rho_L^k}}/{\dbvec{\rho_L^0}}.
\end{equation}

Caso $\rho_L^0 = 0$, então encontramos um ponto inicial factível e o Algoritmo resolve o problema como um método factível o faz. Na prática, porém, essa situação é rara.


No Capítulo \ref{chap:mpis}, Equação~\eqref{eq:infeasible-infty-neig},
mostrou-se que \textcite{Kojima:1993fe}, assim como 
\textcite[pg.~110]{Wright:Primal-dual-interior-point:1997h}, utilizaram em seu
método infactível a vizinhança de passo largo infactível $\Nset_{-\infty}(\ga,\be)$. Nesta definição, pode-se fazer as seguintes ilações:
\begin{itemize}
  \item $x_iz_i$  é não só o par de complementaridade, mas também o resíduo
  atual do sistema KKT na parte da complementaridade -- vide  Equação
  \eqref{eq:KKT-residuals-c}.
  \item O termo  \[\tau = \frac{x^Tz}{n} = \frac{1}{n}\sum_{i=1}^nx_iz_i\]
  pode ser entendido como a média dos resíduos, no que diz respeito à parte
  complementar de KKT. Em nossa notação, $\tau = \dbvec{\rho_{C}}$. Além disso, $x_iz_i = (\rho_{C})_i$.
\end{itemize}

Com tudo isso, mais a Definição \ref{def:residual-vector} e a Proposição \ref{prop:nu_k}, é possível rescrever  a vizinhança
 $\Nset_{-\infty}(\ga,\be)$ para  o sistema KKT escalado
como
\begin{equation}\label{eq:Nset-infty-rho}
\Nset_{-\infty}(\ga,\be) =
 \left\{(x,y,z) \in \Qset^+:\frac{\dbvec{\rho_L}}{\dbvec{\rho^0_L}}    \leq
\beta\frac{\dbvec{\rho_C}}{\dbvec{\rho_C^{0}}}, (\rho_{C})_i \geq
{\ga}\dbvec{\rho_{C}},\forall i =1,\ldots,n  \right\},
\end{equation}
em que $\ga\in(0,1)$ e $\be\geq1$. 

Por conta de \eqref{eq:nu_k} e \eqref{eq:Nset-infty-rho}, segue que para qualquer ponto $(\xk,\yk,\zk)\in \Nset_{-\infty}(\ga,\be) $ vale
\begin{equation}
	\label{eq:nu_k-rho0-rhok}
	 \nu_{k} \leq \beta\dbvec{\rho_C^{k}}/\dbvec{\rho_C^{0}}.
\end{equation}


Até agora, o método proposto resolve o \ac{KKT} escalado \eqref{eq:ScaledKKT}
e além disso usa a função de mérito $\nextphi$ dada em
\eqref{eq:merit-function-al-mu-sig} como guia para a escolha das variáveis
$(\al,\mu,\sig)$  e por conseguinte do próximo ponto. Por conta do exposto
acima, exigir que o próximo ponto pertença à vizinhança $\Nset_{-\infty}(\ga,\be)$, equivale a dar 
uma garantia de que este ponto estará a uma  distância adequada da
trajetória central.

Para tanto, é necessário  escrever funções nas variáveis
$(\al,\mu,\sig)$ que sirvam de restrições para o subproblema de minimização global da
função de mérito $\nextphi$ e que garantam que o ponto escolhido esteja em 
$\Nset_{-\infty}(\ga,\be)$. 

% \begin{teo} 
% \label{thm:polynomial-constraints}

Notando  que $\rho_C = xz$ e portanto $(\rho_C)_i = x_iz_i$ e
$\dbvec{\rho_C} = x^Tz/n$, e usando a Proposição \ref{prop:nu_k} e a equação \eqref{eq:nu_k}, é possível ver que um ponto $(x,y,z)\in\Qset^+$, gerado pelo Algoritmo 
\ref{alg:optimized-choice-of-parameters}, cumpre as condições da vizinhança
$\Nset_{-\infty}(\ga,\be)$ para o sistema \ac{KKT} escalado, se valerem as  desigualdades 
\begin{subequations}
\label{eq:symmetric-polynomials}
\begin{align} 
  (\rho_C)_i(x,y,z) -  \ga\dbvec{\rho_C}(x,y,z)  & \geq 0, \quad \forall i=1,\ldots,n %\leq  \frac{1}{\ga}\dbvec{\rho_C}(x,y,z),
 \label{eq:symmetric-polynomials-b} \\ 
   \dbvec{\rho_C}(x,y,z) - \frac{\nu_{k}}{\be} \dbvec{\rho_C^0}  &\geq 0 .
 \label{eq:symmetric-polynomials-a}
\end{align} 
\end{subequations}


A escolha de  $(\al,\mu,\sig)$ será feita através do seguinte subproblema de
otimização: minimizar a função de mérito para o próximo ponto $\hat\varphi$,
dada pelo Teorema \ref{thm:varphi}, restrita às desigualdades \eqref{eq:symmetric-polynomials}, também para o próximo
para o próximo ponto,  sempre  levando em conta o
teste da razão.








% \section{Subproblema de Otimização de Polinômios}


%De fato, temos o Teorema \ref{thm:explicit-neigb}.

% \begin{teo}	\label{thm:explicit-neigb}
% As condições da vizinhança 	$\Nset_{-\infty}(\ga,\be)$ para o próximo ponto $(\nextx,\nexty,\nextz)$ podem ser escritas em termos de $(\al,\mu\,\sig) $ como 
% \begin{subequations}
% \label{eq:g_C-L-defin}
% \begin{align}
% g_C^i(\al,\mu,\sig)&\geq 0, \quad \forall i=1,\ldots,n \label{eq:g-Ci_defin}\\ 
% g_L(\al,\mu,\sig) &\geq 0\label{eq:g-L_defin}
% \end{align}
% \end{subequations}

% em que
% \begin{subequations}
% \label{eq:g_C-L-explicit}
% \begin{multline}
% \label{eq:g-Ci_explicit}
% g_C^i(\al,\mu,\sig)=      (1-\al)\left((\rho_C)_i - \ga\dbvec{\rho_C} \right) + \al\mu (1-\ga) + \\ +
%  \al(\al-\sig)\left((L_{0,0})_i - \ga \dbvec{L_{0,0}}\right)   + \al^2\left(\La(\mu,\sig)_i  - \ga \dbvec{\La(\mu,\sig)}\right), 
% \end{multline}
% para todo $i=1,\ldots,n$, e 
% \begin{equation}
% \label{eq:g-L_explicit}
% g_L(\al,\mu,\sig)=     (1-\al)\left(\dbvec{\rho_C} -  \be_L \dbvec{\rho_{L}}   \right) + \al\mu  + 
%  \al(\al-\sig) \dbvec{L_{0,0}}  + \al^2 \dbvec{\La(\mu,\sig)}.
% \end{equation}
% \end{subequations}
% em que $\be_{L}= \dfrac{\dbvec{\rho_C^0}}{\be\dbvec{\rho_{L}^{0}}}$. 
% \end{teo}


% \begin{proof} 

Para expressar tal problema de forma apropriada,  assim
como feito com $\nextphi$, vamos explicitar as desigualdades   \eqref{eq:symmetric-polynomials}
 para o próximo ponto em termos de $(\al,\mu,\sig)$.
De fato,  para que o próximo ponto $(\nextx,\nexty,\nextz) $ gerado pelo Algoritmo \ref{alg:optimized-choice-of-parameters}, definimos os lados esquerdos de  \eqref{eq:symmetric-polynomials} como
\begin{subequations}
\label{eq:def-g_C+g_L}
\begin{align}
	g_C^i(\nextx,\nexty,\nextz) & \coloneqq (\nextrho_C(\nextx,\nexty,\nextz))_i - \ga\dbvec{\nextrho_C}(\nextx,\nexty,\nextz), \quad i = 1,\ldots,n, \label{eq:def-g_C}\\ 
	g_L(\nextx,\nexty,\nextz)  & \coloneqq \dbvec{\rho_C}(\nextx,\nexty,\nextz)  - \frac{\nextnu}{\be} \dbvec{\rho_C^0}. \label{eq:def-g_L}
\end{align}
\end{subequations}

Assim, usando os Teoremas \ref{thm:next=residual} e  \ref{thm:varphi} em conjunto com \eqref{eq:def-g_C}, obtemos $g_C^{i}$, para $i=1,\ldots,n$, em função de $(\al,\mu,\sig)$ como 


\begin{subequations}
	\label{eq:g_Ci+G_L-explicit}
\begin{multline}
\label{eq:g-Ci_explicit}
g_C^i(\al,\mu,\sig)=      (1-\al)\left((\rho_C)_i - \ga\dbvec{\rho_C} \right) + \al\mu (1-\ga) + \\ +
 \al(\al-\sig)\left((L_{0,0})_i - \ga \dbvec{L_{0,0}}\right)   + \al^2\left(\La(\mu,\sig)_i  - \ga \dbvec{\La(\mu,\sig)}\right). 
\end{multline}

Por outro lado, considerando que $\nu^{k+1} = \nextnu = (1-\al)\nu$ e levando em conta novamente os Teoremas  \ref{thm:next=residual} e  \ref{thm:varphi} em conjunto com \eqref{eq:def-g_L}, obtemos $g_L$,  também em função de $(\al,\mu,\sig)$ com a seguinte expressão
\begin{equation}
\label{eq:g-L_explicit}
g_L(\al,\mu,\sig) =     (1-\al)\left(\dbvec{\rho_C} -  \be_L \nu   \right) + \al\mu  + 
 \al(\al-\sig) \dbvec{L_{0,0}}  + \al^2 \dbvec{\La(\mu,\sig)},
\end{equation}
\end{subequations}
 em que $\be_{L}= \dfrac{\dbvec{\rho_C^0}}{\be}$. 



Desta forma, exigindo que as desigualdades 
\begin{align*}
	g_C^i(\al,\mu,\sig) &\geq 0 \quad \forall i = 1,\ldots,n \\% \label{eq:ineq-g_C}\\ 
	g_L(\al,\mu,\sig)  & \geq 0 %\label{eq:ineq-g_L}
\end{align*}
sejam satisfeitas para alguma tripla $(\al,\mu,\sig) $, garantimos que o próximo ponto estará na vizinhança $\Nset_{-\infty}(\ga,\be)$. Neste sentido, essas desigualdades se tornam parte das restrições de nosso subproblema de otimização global, o qual leva a escolha, em cada iteração, de $(\al,\mu,\sig)$.



Resumidamente, o subproblema de otimização global pode ser escrito como
\begin{equation}
	\begin{array}{lc}
\displaystyle \min_{(\al,\mu,\si)} & \hat\varphi(\al,\mu,\si) \\
\text{s. a.} &\begin{cases} g_C^i(\al,\mu,\sig) \geq 0 \quad \forall i = 1,\ldots,n \\
				g_L(\al,\mu,\sig)   \geq 0 	\\
				 0\leq (\al,\mu,\si) \leq u,
				 	
				 \end{cases}\
\end{array}
\label{eq:pop-subproblem}
\end{equation}
em que   $u\in\Real^3$ é um vetor de limitantes superiores para  $(\al,\mu,\sig)$. Para
$\al$, $u_1<1$, pois  não queremos que o próximo ponto saia do interior do ortante positivo. Com efeito, também não queremos $\al=0$ e por isso mesmo,  durante a
demonstração de convergência dar-se-á garantia de que é possível reduzir a
função de mérito usando $\al>0$. Com efeito, o teste da razão é sempre executado em cada iteração para garantir isso.

Por outro lado, para $\mu$ e $\sig$ os limitantes superiores $u_{2}$ e $u_{3}$ são escalares positivos que podem ou não ser diferente para esses dois parâmetros.

 
O Algoritmo \ref{alg:optimized-choice-of-parameters} resume o método proposto neste capítulo. A escolha do ponto inicial e do critério de parada, serão descritas no Capítulo \ref{chap:numerical}, junto aos resultados numéricos.

\begin{algorithm}
\onehalfspacing
\caption{Método de Escolha Otimizada de Parâmetros.}
\label{alg:optimized-choice-of-parameters} \begin{algorithmic}[1]
\Procedure{ResolveLP}{$A,b,c$}
\State $k \gets 0$
\State $(x^0,y^0,z^0) \gets$ \Call{PontoInicial}{$A,b,c$}.
\Comment{Assegure que  $(x^0,z^0)>0$.}
	\Repeat
		\State Resolva   \eqref{eq:affine-scaling-system} e encontre
		$((\dex)^{k},(\dey)^{k},(\dez)^{k})$.
		\State 	Encontre $((\Dex^\mu)^{k},(\Dey^\mu)^{k},(\Dez^\mu)^{k})$ e
		$((\Dex^\sig)^{k},(\Dey^\sig)^{k},(\Dez^\sig)^{k})$ utilizando
		\eqref{eq:De_C}.
		\State Calcule os coeficientes de  $\nextphi(\al,\mu,\sig)$ usando
		\eqref{eq:merit-function-al-mu-sig} 
		\State Calcule os coeficienes de $g_{L}(\al,\mu,\sig)$ e $g^{i}_{C}(\al,\mu,\sig)$ utilizando \eqref{eq:g_Ci+G_L-explicit}.
		\State Encontre $(\al_{k},\mu_{k},\sig_{k})$ resolvendo o subproblema de
		otimização global 
		\eqref{eq:pop-subproblem}.
		\State Garanta que $\al_k\in(0,1)$ tal que $(x^{k+1},z^{k+1})>0$ usando \eqref{eq:ratio-test} e faça
		\[
		\begin{aligned}	
		& x^{k+1} = x^{k} + \al_k((\dex)^{k} + \mu_{k}(\Dex^\mu)^{k} +
		\sig_{k}(\Dex^\sig)^{k} )
		\\
		& y^{k+1} = y^{k} + \al_k((\dey)^{k} + \mu_{k}(\Dey^\mu)^{k} +
		\sig_{k}(\Dey^\sig)^{k} )
		\\
		& z^{k+1} = z^{k} + \al_k((\dez)^{k} + \mu_{k}(\Dez^\mu)^{k} +
		\sig_{k}(\Dez^\sig)^{k} ) \end{aligned}. 
		\]		
		\State $k\gets k+1$
	\Until{O critério de parada ser satisfeito.}
\EndProcedure
\end{algorithmic}
\end{algorithm}
  
 No próximo capítulo estabeleceremos os resultados teóricos dessa abordagem, garantindo convergência e polinomialidade do algoritmo, em relação a diminuição da função de mérito e da garantia de que o próximo ponto esteja na vizinhança $\Nset_{-\infty}$.
 
